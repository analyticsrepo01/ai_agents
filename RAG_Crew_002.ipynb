{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267fa872-4102-44f4-b3f3-9cfdf97ac798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## RAG implemetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b475e6-68e3-4075-8f99-92e71fe245d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "LOCATION=\"asia-southeast1\"\n",
    "\n",
    "FOLDER_NAME=\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafe0c4d-659f-43ad-900c-c74bbbb69970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import tool\n",
    "# from langchain_vertexai import ChatGemini\n",
    "from crewai_tools.tools import FileReadTool\n",
    "import os, requests, re, mdpdf, subprocess\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "import uuid, os\n",
    "\n",
    "# Initialize Gemini LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-pro\", # Replace with your desired Gemini model\n",
    "    project_id=os.getenv(PROJECT_ID), # Your Vertex AI project ID\n",
    "    location=\"us-central1\", # Your Vertex AI location\n",
    ")\n",
    "\n",
    "\n",
    "def generate_pro(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    full_prompt = '''summarize the prompt below and do note prompt below will be send to imagen mode so please clean up any sensitve words and replace them into unblocked words like replace girl or lady can be replaced by female human and so on : ''' + input_prompt\n",
    "    responses = model.generate_content(\n",
    "    input_prompt,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 8190,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1\n",
    "    },stream=False,)\n",
    "    \n",
    "    # print (responses.text)\n",
    "    \n",
    "    return(responses.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc045b6a-f6d0-4022-8c78-f0c8fb7a6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools import tool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import requests, os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# embedding_function = OpenAIEmbeddings()\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "# Tool 1 : Save the news articles in a database\n",
    "class SearchNewsDB:\n",
    "    @tool(\"News DB Tool\")\n",
    "    def news(query: str):\n",
    "        \"\"\"Fetch news articles and process their contents.\"\"\"\n",
    "        API_KEY = os.getenv('NEWSAPI_KEY')  # Fetch API key from environment variable\n",
    "        base_url = \"https://newsapi.org/v2/everything\"\n",
    "        \n",
    "        params = {\n",
    "            'q': query,\n",
    "            'sortBy': 'publishedAt',\n",
    "            'apiKey': API_KEY,\n",
    "            'language': 'en',\n",
    "            'pageSize': 5,\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            return \"Failed to retrieve news.\"\n",
    "        \n",
    "        articles = response.json().get('articles', [])\n",
    "        all_splits = []\n",
    "        for article in articles:\n",
    "            # Assuming WebBaseLoader can handle a list of URLs\n",
    "            loader = WebBaseLoader(article['url'])\n",
    "            docs = loader.load()\n",
    "\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            splits = text_splitter.split_documents(docs)\n",
    "            all_splits.extend(splits)  # Accumulate splits from all articles\n",
    "\n",
    "        # Index the accumulated content splits if there are any\n",
    "        if all_splits:\n",
    "            vectorstore = Chroma.from_documents(all_splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n",
    "            retriever = vectorstore.similarity_search(query)\n",
    "            return retriever\n",
    "        else:\n",
    "            return \"No content available for processing.\"\n",
    "\n",
    "# Tool 2 : Get the news articles from the database\n",
    "class GetNews:\n",
    "    @tool(\"Get News Tool\")\n",
    "    def news(query: str) -> str:\n",
    "        \"\"\"Search Chroma DB for relevant news information based on a query.\"\"\"\n",
    "        vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "        retriever = vectorstore.similarity_search(query)\n",
    "        return retriever\n",
    "\n",
    "# Tool 3 : Search for news articles on the web\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "# 2. Creating Agents\n",
    "news_search_agent = Agent(\n",
    "    role='News Seacher',\n",
    "    goal='Generate key points for each news article from the latest news',\n",
    "    backstory='Expert in analysing and generating key points from news content for quick updates.',\n",
    "    tools=[SearchNewsDB().news],\n",
    "    allow_delegation=True,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "writer_agent = Agent(\n",
    "    role='Writer',\n",
    "    goal='Identify all the topics received. Use the Get News Tool to verify the each topic to search. Use the Search tool for detailed exploration of each topic. Summarise the retrieved information in depth for every topic.',\n",
    "    backstory='Expert in crafting engaging narratives from complex information.',\n",
    "    tools=[GetNews().news, search_tool],\n",
    "    allow_delegation=True,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# 3. Creating Tasks\n",
    "news_search_task = Task(\n",
    "    description='Search for AI 2024 and create key points for each news.',\n",
    "    agent=news_search_agent,\n",
    "    tools=[SearchNewsDB().news]\n",
    ")\n",
    "\n",
    "writer_task = Task(\n",
    "    description=\"\"\"\n",
    "    Go step by step.\n",
    "    Step 1: Identify all the topics received.\n",
    "    Step 2: Use the Get News Tool to verify the each topic by going through one by one.\n",
    "    Step 3: Use the Search tool to search for information on each topic one by one. \n",
    "    Step 4: Go through every topic and write an in-depth summary of the information retrieved.\n",
    "    Don't skip any topic.\n",
    "    \"\"\",\n",
    "    agent=writer_agent,\n",
    "    context=[news_search_task],\n",
    "    tools=[GetNews().news, search_tool]\n",
    ")\n",
    "\n",
    "# 4. Creating Crew\n",
    "news_crew = Crew(\n",
    "    agents=[news_search_agent, writer_agent],\n",
    "    tasks=[news_search_task, writer_task],\n",
    "    process=Process.sequential, \n",
    "    manager_llm=llm\n",
    ")\n",
    "\n",
    "# Execute the crew to see RAG in action\n",
    "result = news_crew.kickoff()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py312",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "py312 (Local)",
   "language": "python",
   "name": "conda-root-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
