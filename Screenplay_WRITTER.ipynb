{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c2061f-d13f-46dc-8e5c-6abffae27ccd",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "Example script to automatically write a screenplay from a newsgroup post using agents with Crew.ai (https://github.com/joaomdmoura/crewAI)\n",
    "You can also try it out with a personal email with many replies back and forth and see it turn into a movie script.\n",
    "Demonstrates:\n",
    "- multiple API endpoints (offical VERTEX AI)\n",
    "- running single tasks: spam detection and scoring\n",
    "- running a crew to create a screenplay from a newsgroup post by first analyzing the text, creating a dialogue and ultimately formatting it\n",
    "Additional endpoints requirements:\n",
    "  pip install langchain_VERTEXAI\n",
    "Author: SAURABH MANGAL\n",
    "License: MIT\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4313842-68ce-4757-b357-ac16daa74bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -U 'crewai[tools]' mdpdf \n",
    "# %pip install --upgrade --quiet  langchain-core langchain-google-vertexai\n",
    "# %pip install mdpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6be2717-ec6e-42a7-9112-d2792d21dec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "LOCATION=\"asia-southeast1\"\n",
    "\n",
    "FOLDER_NAME=\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018f20e5-cb33-49f1-b3f2-85469fb7ac99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import tool\n",
    "# from langchain_vertexai import ChatGemini\n",
    "from crewai_tools.tools import FileReadTool\n",
    "import os, requests, re, mdpdf, subprocess\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "import uuid, os\n",
    "\n",
    "# Initialize Gemini LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-pro\", # Replace with your desired Gemini model\n",
    "    project_id=os.getenv(PROJECT_ID), # Your Vertex AI project ID\n",
    "    location=\"us-central1\", # Your Vertex AI location\n",
    ")\n",
    "\n",
    "\n",
    "def generate_pro(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    full_prompt = '''summarize the prompt below and do note prompt below will be send to imagen mode so please clean up any sensitve words and replace them into unblocked words like replace girl or lady can be replaced by female human and so on : ''' + input_prompt\n",
    "    responses = model.generate_content(\n",
    "    input_prompt,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 8190,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1\n",
    "    },stream=False,)\n",
    "    \n",
    "    # print (responses.text)\n",
    "    \n",
    "    return(responses.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbdaa477-46b3-4c58-9e1a-cef801652fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 15:20:23,476 - 139684214273856 - __init__.py-__init__:521 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mFinal Answer: STOP STOP\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This spam message will be filtered out\n",
      "\u001b[1m\u001b[93m [DEBUG]: == Working Agent: analyse\u001b[00m\n",
      "\u001b[1m\u001b[93m [INFO]: == Starting Task: Analyse in much detail the following discussion:\n",
      "### DISCUSSION:\n",
      "From: keith@cco.caltech.edu (Keith Allan Schneider)\n",
      "Subject: Re: <Political Atheists?\n",
      "Organization: California Institute of Technology, Pasadena\n",
      "Lines: 50\n",
      "NNTP-Posting-Host: punisher.caltech.edu\n",
      "\n",
      "bobbe@vice.ICO.TEK.COM (Robert Beauchaine) writes:\n",
      "\n",
      ">>I think that about 70% (or so) people approve of the\n",
      ">>death penalty, even realizing all of its shortcomings.  Doesn't this make\n",
      ">>it reasonable?  Or are *you* the sole judge of reasonability?\n",
      ">Aside from revenge, what merits do you find in capital punishment?\n",
      "\n",
      "Are we talking about me, or the majority of the people that support it?\n",
      "Anyway, I think that \"revenge\" or \"fairness\" is why most people are in\n",
      "favor of the punishment.  If a murderer is going to be punished, people\n",
      "that think that he should \"get what he deserves.\"  Most people wouldn't\n",
      "think it would be fair for the murderer to live, while his victim died.\n",
      "\n",
      ">Revenge?  Petty and pathetic.\n",
      "\n",
      "Perhaps you think that it is petty and pathetic, but your views are in the\n",
      "minority.\n",
      "\n",
      ">We have a local televised hot topic talk show that very recently\n",
      ">did a segment on capital punishment.  Each and every advocate of\n",
      ">the use of this portion of our system of \"jurisprudence\" cited the\n",
      ">main reason for supporting it:  \"That bastard deserved it\".  True\n",
      ">human compassion, forgiveness, and sympathy.\n",
      "\n",
      "Where are we required to have compassion, forgiveness, and sympathy?  If\n",
      "someone wrongs me, I will take great lengths to make sure that his advantage\n",
      "is removed, or a similar situation is forced upon him.  If someone kills\n",
      "another, then we can apply the golden rule and kill this person in turn.\n",
      "Is not our entire moral system based on such a concept?\n",
      "\n",
      "Or, are you stating that human life is sacred, somehow, and that it should\n",
      "never be violated?  This would sound like some sort of religious view.\n",
      " \n",
      ">>I mean, how reasonable is imprisonment, really, when you think about it?\n",
      ">>Sure, the person could be released if found innocent, but you still\n",
      ">>can't undo the imiprisonment that was served.  Perhaps we shouldn't\n",
      ">>imprision people if we could watch them closely instead.  The cost would\n",
      ">>probably be similar, especially if we just implanted some sort of\n",
      ">>electronic device.\n",
      ">Would you rather be alive in prison or dead in the chair?  \n",
      "\n",
      "Once a criminal has committed a murder, his desires are irrelevant.\n",
      "\n",
      "And, you still have not answered my question.  If you are concerned about\n",
      "the death penalty due to the possibility of the execution of an innocent,\n",
      "then why isn't this same concern shared with imprisonment.  Shouldn't we,\n",
      "by your logic, administer as minimum as punishment as possible, to avoid\n",
      "violating the liberty or happiness of an innocent person?\n",
      "\n",
      "keith\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Content roles do not match: model != ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 199\u001b[0m\n\u001b[1;32m    179\u001b[0m task3 \u001b[38;5;241m=\u001b[39m Task(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mFormat the script exactly like this:\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124m  ## (person 1):\u001b[39m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124m  (first text line from person 1)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124m  \u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'''\u001b[39m, agent\u001b[38;5;241m=\u001b[39mformatter, expected_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOP\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m    193\u001b[0m crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[1;32m    194\u001b[0m   agents\u001b[38;5;241m=\u001b[39m[analyst, scriptwriter,formatter],\n\u001b[1;32m    195\u001b[0m   tasks\u001b[38;5;241m=\u001b[39m[task1, task2, task3],\n\u001b[1;32m    196\u001b[0m   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;66;03m# Crew verbose more will let you know what tasks are being worked on, you can set it to 1 or 2 to different logging levels\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   process\u001b[38;5;241m=\u001b[39mProcess\u001b[38;5;241m.\u001b[39msequential \u001b[38;5;66;03m# Sequential process will have tasks executed one after the other and the outcome of the previous one is passed as extra content into this next.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m )\n\u001b[0;32m--> 199\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#get rid of directions and actions between brackets, eg: (smiling)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m result \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(.*?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/crew.py:204\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    201\u001b[0m metrics \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 204\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    206\u001b[0m     result, manager_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/crew.py:240\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== Working Agent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrole\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbold_yellow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== Starting Task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbold_yellow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m )\n\u001b[0;32m--> 240\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39masync_execution:\n\u001b[1;32m    242\u001b[0m     task_output \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/task.py:148\u001b[0m, in \u001b[0;36mTask.execute\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/task.py:157\u001b[0m, in \u001b[0;36mTask._execute\u001b[0;34m(self, agent, task, context, tools)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent, task, context, tools):\n\u001b[0;32m--> 157\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     exported_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[1;32m    166\u001b[0m         description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m    167\u001b[0m         exported_output\u001b[38;5;241m=\u001b[39mexported_output,\n\u001b[1;32m    168\u001b[0m         raw_output\u001b[38;5;241m=\u001b[39mresult,\n\u001b[1;32m    169\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/agent.py:193\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_description \u001b[38;5;241m=\u001b[39m render_text_description(tools)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__tools_names(tools)\n\u001b[0;32m--> 193\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller\u001b[38;5;241m.\u001b[39mstop_rpm_counter()\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/agents/executor.py:64\u001b[0m, in \u001b[0;36mCrewAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations, time_elapsed):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit():\n\u001b[0;32m---> 64\u001b[0m         next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_callback:\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_callback(next_step_output)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain/agents/agent.py:1097\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1095\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1097\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1107\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/agents/executor.py:118\u001b[0m, in \u001b[0;36mCrewAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    116\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain/agents/agent.py:387\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a streaming\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# fashion to make it possible to get access to the individual LLM tokens\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# when using stream_log with the Agent Executor.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:2685\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   2680\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   2682\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2683\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2684\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2685\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:2672\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   2667\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2668\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   2669\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2671\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2672\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   2673\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2674\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m   2675\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   2676\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2677\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:1743\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:2636\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m   2628\u001b[0m     final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   2629\u001b[0m         final_pipeline,\n\u001b[1;32m   2630\u001b[0m         patch_config(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2633\u001b[0m         ),\n\u001b[1;32m   2634\u001b[0m     )\n\u001b[0;32m-> 2636\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_pipeline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:1209\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m final: Input\n\u001b[1;32m   1207\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgot_first_val\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madapt_first_streaming_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:4532\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   4527\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4528\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   4529\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4530\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   4531\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 4532\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   4533\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4535\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4536\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:1226\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1220\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed while trying to add together \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1221\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(final)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1222\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese types should be addable for transform to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1223\u001b[0m             )\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:239\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    233\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[1;32m    234\u001b[0m         e,\n\u001b[1;32m    235\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[1;32m    236\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    237\u001b[0m         ),\n\u001b[1;32m    238\u001b[0m     )\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:222\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_gen_info_and_msg_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:527\u001b[0m, in \u001b[0;36mChatVertexAI._stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m telemetry\u001b[38;5;241m.\u001b[39mtool_context_manager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_agent):\n\u001b[1;32m    520\u001b[0m     responses \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msend_message(\n\u001b[1;32m    521\u001b[0m         message,\n\u001b[1;32m    522\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    525\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    526\u001b[0m     )\n\u001b[0;32m--> 527\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_parse_response_candidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:933\u001b[0m, in \u001b[0;36mChatSession._send_message_streaming\u001b[0;34m(self, content, generation_config, safety_settings, tools)\u001b[0m\n\u001b[1;32m    931\u001b[0m chunks\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_response:\n\u001b[0;32m--> 933\u001b[0m     \u001b[43m_append_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     full_response \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:1776\u001b[0m, in \u001b[0;36m_append_response\u001b[0;34m(base_response, new_response)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_append_response\u001b[39m(\n\u001b[1;32m   1773\u001b[0m     base_response: GenerationResponse,\n\u001b[1;32m   1774\u001b[0m     new_response: GenerationResponse,\n\u001b[1;32m   1775\u001b[0m ):\n\u001b[0;32m-> 1776\u001b[0m     \u001b[43m_append_gapic_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:1796\u001b[0m, in \u001b[0;36m_append_gapic_response\u001b[0;34m(base_response, new_response)\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base_response\u001b[38;5;241m.\u001b[39mcandidates[idx]\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m!=\u001b[39m idx:\n\u001b[1;32m   1793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1794\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect base candidate ordering: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_response\u001b[38;5;241m.\u001b[39mcandidates\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1795\u001b[0m         )\n\u001b[0;32m-> 1796\u001b[0m     \u001b[43m_append_gapic_candidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(base_response\u001b[38;5;241m.\u001b[39mcandidates):\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:1818\u001b[0m, in \u001b[0;36m_append_gapic_candidate\u001b[0;34m(base_candidate, new_candidate)\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_candidate\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m!=\u001b[39m new_candidate\u001b[38;5;241m.\u001b[39mindex:\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1815\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect candidate indexes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_candidate\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_candidate\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1816\u001b[0m     )\n\u001b[0;32m-> 1818\u001b[0m \u001b[43m_append_gapic_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_candidate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_candidate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;66;03m# For these attributes, the last value wins\u001b[39;00m\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_candidate\u001b[38;5;241m.\u001b[39mfinish_reason:\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:1836\u001b[0m, in \u001b[0;36m_append_gapic_content\u001b[0;34m(base_content, new_content)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_append_gapic_content\u001b[39m(\n\u001b[1;32m   1832\u001b[0m     base_content: gapic_content_types\u001b[38;5;241m.\u001b[39mContent,\n\u001b[1;32m   1833\u001b[0m     new_content: gapic_content_types\u001b[38;5;241m.\u001b[39mContent,\n\u001b[1;32m   1834\u001b[0m ):\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base_content\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m!=\u001b[39m new_content\u001b[38;5;241m.\u001b[39mrole:\n\u001b[0;32m-> 1836\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1837\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent roles do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_content\u001b[38;5;241m.\u001b[39mrole\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_content\u001b[38;5;241m.\u001b[39mrole\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1838\u001b[0m         )\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part_idx, part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_content\u001b[38;5;241m.\u001b[39mparts):\n\u001b[1;32m   1841\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m part_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(base_content\u001b[38;5;241m.\u001b[39mparts):\n",
      "\u001b[0;31mValueError\u001b[0m: Content roles do not match: model != "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "# from langchain.chat_models import openai\n",
    "#endpoint specific imports\n",
    "# import langchain_mistralai\n",
    "# from langchain_mistralai.chat_models import ChatMistralAI\n",
    "# from langchain_community.llms import Together\n",
    "# from langchain_community.chat_models import ChatAnyscale\n",
    "\n",
    "\n",
    "## Choose here which API endpoint to use, uncomment only one:\n",
    "# Official Mistral: benefit of having access to mistral-medium\n",
    "# Together.ai: lots of models to choose from\n",
    "# Anyscale: cheapest at the time of writing\n",
    "#endpoint = 'mistral_official'\n",
    "#endpoint = 'togetherai'\n",
    "endpoint = 'mistral_official'\n",
    "\n",
    "#put you API keys here\n",
    "# mistral_key = ''\n",
    "# togetherai_key = ''\n",
    "# anyscale_key = ''\n",
    "\n",
    "#model choice: i already have good results with mistralai/Mistral-7B-Instruct-v0.2\n",
    "\n",
    "# if endpoint == 'mistral_official':\n",
    "#   mixtral=ChatMistralAI(mistral_api_key=mistral_key, model=\"mistral-tiny\",temperature=0.6)\n",
    "# elif endpoint == 'togetherai':\n",
    "#   #i get timeouts using Together() , so i use ChatOpenAI() instead\n",
    "#   #mixtral = Together(model=\"mistralai/Mistral-7B-Instruct-v0.2\", together_api_key=togetherai_key ) #or mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "#   mixtral= openai.ChatOpenAI(base_url=\"https://api.together.xyz/v1\", api_key=togetherai_key, temperature=0.5,  model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# elif endpoint == 'anyscale':\n",
    "#   mixtral = ChatAnyscale(model='mistralai/Mistral-7B-Instruct-v0.1',  api_key=anyscale_key,  streaming=False)\n",
    " \n",
    "\n",
    "## Define Agents\n",
    "spamfilter = Agent(\n",
    "  role='spamfilter',\n",
    "  goal='''Decide whether a text is spam or not.''',\n",
    "  backstory='You are an expert spam filter with years of experience. You DETEST advertisements, newsletters and vulgar language.',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    ")\n",
    "\n",
    "analyst = Agent(\n",
    "  role='analyse',\n",
    "  goal='''You will distill all arguments from all discussion members. Identify who said what. You can reword what they said as long as the main discussion points remain.''',\n",
    "  backstory='You are an expert discussion analyst.',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    ")\n",
    "\n",
    "scriptwriter = Agent(\n",
    "  role='scriptwriter',\n",
    "  goal='Turn a conversation into a movie script. Only write the dialogue parts. Do not start the sentence with an action. Do not specify situational descriptions. Do not write parentheticals.',\n",
    "  backstory='''You are an expert on writing natural sounding movie script dialogues. You only focus on the text part and you HATE directional notes.''',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    "\n",
    ")\n",
    "\n",
    "formatter = Agent(\n",
    "  role='formatter',\n",
    "  goal='''Format the text as asked. Leave out actions from discussion members that happen between brackets, eg (smiling).''',\n",
    "  backstory='You are an expert text formatter.',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    ")\n",
    "\n",
    "scorer = Agent(\n",
    "  role='scorer',\n",
    "  goal='''You score a dialogue assessing various aspects of the exchange between the participants using a 1-10 scale, where 1 is the lowest performance and 10 is the highest:\n",
    "  Scale:\n",
    "  1-3: Poor - The dialogue has significant issues that prevent effective communication.\n",
    "  4-6: Average - The dialogue has some good points but also has notable weaknesses.\n",
    "  7-9: Good - The dialogue is mostly effective with minor issues.\n",
    "  10: Excellent - The dialogue is exemplary in achieving its purpose with no apparent issues.\n",
    "  Factors to Consider:\n",
    "  Clarity: How clear is the exchange? Are the statements and responses easy to understand?\n",
    "  Relevance: Do the responses stay on topic and contribute to the conversation's purpose?\n",
    "  Conciseness: Is the dialogue free of unnecessary information or redundancy?\n",
    "  Politeness: Are the participants respectful and considerate in their interaction?\n",
    "  Engagement: Do the participants seem interested and actively involved in the dialogue?\n",
    "  Flow: Is there a natural progression of ideas and responses? Are there awkward pauses or interruptions?\n",
    "  Coherence: Does the dialogue make logical sense as a whole?\n",
    "  Responsiveness: Do the participants address each other's points adequately?\n",
    "  Language Use: Is the grammar, vocabulary, and syntax appropriate for the context of the dialogue?\n",
    "  Emotional Intelligence: Are the participants aware of and sensitive to the emotional tone of the dialogue?\n",
    "  ''',\n",
    "  backstory='You are an expert at scoring conversations on a scale of 1 to 10.',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    ")\n",
    "\n",
    "\n",
    "#this is one example of a public post in the newsgroup alt.atheism\n",
    "#try it out yourself by replacing this with your own email thread or text or ...\n",
    "discussion = '''From: keith@cco.caltech.edu (Keith Allan Schneider)\n",
    "Subject: Re: <Political Atheists?\n",
    "Organization: California Institute of Technology, Pasadena\n",
    "Lines: 50\n",
    "NNTP-Posting-Host: punisher.caltech.edu\n",
    "\n",
    "bobbe@vice.ICO.TEK.COM (Robert Beauchaine) writes:\n",
    "\n",
    ">>I think that about 70% (or so) people approve of the\n",
    ">>death penalty, even realizing all of its shortcomings.  Doesn't this make\n",
    ">>it reasonable?  Or are *you* the sole judge of reasonability?\n",
    ">Aside from revenge, what merits do you find in capital punishment?\n",
    "\n",
    "Are we talking about me, or the majority of the people that support it?\n",
    "Anyway, I think that \"revenge\" or \"fairness\" is why most people are in\n",
    "favor of the punishment.  If a murderer is going to be punished, people\n",
    "that think that he should \"get what he deserves.\"  Most people wouldn't\n",
    "think it would be fair for the murderer to live, while his victim died.\n",
    "\n",
    ">Revenge?  Petty and pathetic.\n",
    "\n",
    "Perhaps you think that it is petty and pathetic, but your views are in the\n",
    "minority.\n",
    "\n",
    ">We have a local televised hot topic talk show that very recently\n",
    ">did a segment on capital punishment.  Each and every advocate of\n",
    ">the use of this portion of our system of \"jurisprudence\" cited the\n",
    ">main reason for supporting it:  \"That bastard deserved it\".  True\n",
    ">human compassion, forgiveness, and sympathy.\n",
    "\n",
    "Where are we required to have compassion, forgiveness, and sympathy?  If\n",
    "someone wrongs me, I will take great lengths to make sure that his advantage\n",
    "is removed, or a similar situation is forced upon him.  If someone kills\n",
    "another, then we can apply the golden rule and kill this person in turn.\n",
    "Is not our entire moral system based on such a concept?\n",
    "\n",
    "Or, are you stating that human life is sacred, somehow, and that it should\n",
    "never be violated?  This would sound like some sort of religious view.\n",
    " \n",
    ">>I mean, how reasonable is imprisonment, really, when you think about it?\n",
    ">>Sure, the person could be released if found innocent, but you still\n",
    ">>can't undo the imiprisonment that was served.  Perhaps we shouldn't\n",
    ">>imprision people if we could watch them closely instead.  The cost would\n",
    ">>probably be similar, especially if we just implanted some sort of\n",
    ">>electronic device.\n",
    ">Would you rather be alive in prison or dead in the chair?  \n",
    "\n",
    "Once a criminal has committed a murder, his desires are irrelevant.\n",
    "\n",
    "And, you still have not answered my question.  If you are concerned about\n",
    "the death penalty due to the possibility of the execution of an innocent,\n",
    "then why isn't this same concern shared with imprisonment.  Shouldn't we,\n",
    "by your logic, administer as minimum as punishment as possible, to avoid\n",
    "violating the liberty or happiness of an innocent person?\n",
    "\n",
    "keith\n",
    "'''\n",
    "\n",
    "# Filter out spam and vulgar posts\n",
    "task0 = Task(description='''Read the following newsgroup post. If this contains vulgar language reply with STOP . If this is spam reply with STOP. NEWGROUP POST:\\n''' + discussion, agent=spamfilter,\n",
    "            expected_output=\"\"\n",
    "            )\n",
    "result = task0.execute()\n",
    "if \"STOP\" in result:\n",
    "    #stop here and proceed to next post\n",
    "    print('This spam message will be filtered out')\n",
    "\n",
    "# process post with a crew of agents, ultimately delivering a well formatted dialogue\n",
    "task1 = Task(description='''Analyse in much detail the following discussion:\\n### DISCUSSION:\\n''' + discussion, agent=analyst,\n",
    "            expected_output=\"STOP\"  # Add the expected_output field.\n",
    "            )\n",
    "task2 = Task(description='Create a dialogue heavy screenplay from the discussion, between two persons. Do NOT write parentheticals. Leave out wrylies. You MUST SKIP directional notes.', agent=scriptwriter, \n",
    "           expected_output=\"STOP\"  # Add the expected_output field. \n",
    "            )\n",
    "task3 = Task(description='''Format the script exactly like this:\n",
    "  ## (person 1):\n",
    "  (first text line from person 1)\n",
    "             \n",
    "  ## (person 2):\n",
    "  (first text line from person 2)\n",
    "             \n",
    "  ## (person 1):\n",
    "  (second text line from person 1)\n",
    "             \n",
    "  ## (person 2):\n",
    "  (second text line from person 2)\n",
    "  \n",
    "  ''', agent=formatter, expected_output=\"STOP\" )\n",
    "crew = Crew(\n",
    "  agents=[analyst, scriptwriter,formatter],\n",
    "  tasks=[task1, task2, task3],\n",
    "  verbose=2, # Crew verbose more will let you know what tasks are being worked on, you can set it to 1 or 2 to different logging levels\n",
    "  process=Process.sequential # Sequential process will have tasks executed one after the other and the outcome of the previous one is passed as extra content into this next.\n",
    ")\n",
    "result = crew.kickoff()\n",
    "\n",
    "#get rid of directions and actions between brackets, eg: (smiling)\n",
    "result = re.sub(r'\\(.*?\\)', '', result)\n",
    "\n",
    "print('===================== end result from crew ===================================')\n",
    "print(result)\n",
    "print('===================== score ==================================================')\n",
    "task4 = Task(description='Read the following dialogue. Then score the script on a scale of 1 to 10. Only give the score as a number, nothing else. Do not give an explanation.\\n'+result, agent=scorer, \n",
    "           expected_output=\"STOP\"  )\n",
    "score = task4.execute()\n",
    "score = score.split('\\n')[0]  #sometimes an explanation comes after score, ignore\n",
    "print(f'Scoring the dialogue as: {score}/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649b571-0ded-428e-a298-a398a180c07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py312",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "py312 (Local)",
   "language": "python",
   "name": "conda-root-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
