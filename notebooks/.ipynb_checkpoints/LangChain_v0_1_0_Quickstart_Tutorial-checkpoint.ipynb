{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW_wLiIUjMv-"
   },
   "source": [
    "# LangChain 0.1.0 Quickstart - Create Your Own RAG Chains\n",
    "\n",
    "Welcome to this Quickstart walkthrough of the new version of Langchain. This tutorial follows the official Langchain documentation quickstart. I made a video where I go through exactly all the code in this notebook 👉 https://youtu.be/LBNpyjcbv0o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxslo5rjgTfS"
   },
   "source": [
    "# 1.0 Install the required packages\n",
    "\n",
    "Some packages have changed inside Langchain. This is how you should install and import them from now on. Remember that langchain has been divided into three main packages:\n",
    "- `langchain` contains pre-built chains and other stuff you may need.\n",
    "- `langchain-core` contains the main abstractions of the library.\n",
    "- `langchain-community` contains third-party integrations like Chroma, FAISS, etc.\n",
    "- Some third-party integrations have their own package (outside `langchain-community`) that you should install if you want to use them. For example, OpenAI has its own package: `langchain-openai`.\n",
    "\n",
    "All three are installed when you run `pip install langchain`. But you can install community or core individually as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EfpEJfXIa_f",
    "outputId": "a75027a2-3b52-4aeb-cf02-111711dc3322",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "LOCATION=\"asia-southeast1\"\n",
    "\n",
    "FOLDER_NAME=\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_gy2VgCqVHT8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "import os\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import tool\n",
    "# from langchain_vertexai import ChatGemini\n",
    "from crewai_tools.tools import FileReadTool\n",
    "import os, requests, re, mdpdf, subprocess\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "import uuid, os\n",
    "\n",
    "# Initialize Gemini LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-pro\", # Replace with your desired Gemini model\n",
    "    project_id=os.getenv(PROJECT_ID), # Your Vertex AI project ID\n",
    "    location=\"us-central1\", # Your Vertex AI location\n",
    ")\n",
    "\n",
    "\n",
    "def generate_pro(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    full_prompt = '''summarize the prompt below and do note prompt below will be send to imagen mode so please clean up any sensitve words and replace them into unblocked words like replace girl or lady can be replaced by female human and so on : ''' + input_prompt\n",
    "    responses = model.generate_content(\n",
    "    input_prompt,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 8190,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1\n",
    "    },stream=False,)\n",
    "    \n",
    "    # print (responses.text)\n",
    "    \n",
    "    return(responses.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZjTvnJPVR7H",
    "outputId": "ad678b23-ce71-4acf-8495-b9727cd5d9d6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"**What is Docker?**\\n\\nDocker is an open-source platform that uses containerization technology to package, distribute, and run applications in isolated environments called containers. A container bundles the application code, its dependencies, and its runtime environment into a single executable unit.\\n\\n**How Docker is Useful for Deployment:**\\n\\nDocker simplifies deployment in several ways:\\n\\n**1. Isolation and Consistency:**\\n* Containers provide isolated environments, ensuring that applications run without interfering with each other or the host system.\\n* This consistency ensures that applications behave the same in different environments, from development to production.\\n\\n**2. Rapid Deployment:**\\n* Containers are lightweight and portable, enabling fast and efficient deployment.\\n* They can be easily copied, moved, and launched across different servers or cloud platforms.\\n\\n**3. Scalability and High Availability:**\\n* Docker allows easy scaling of applications by deploying multiple instances of a container.\\n* Containers can be easily replaced or updated without downtime, ensuring high availability.\\n\\n**4. Version Control:**\\n* Docker images (the packaged containers) can be versioned and managed using tools like Docker Hub.\\n* This enables teams to track changes, roll back to previous versions, and collaborate effectively.\\n\\n**5. Infrastructure Agnostic:**\\n* Containers can run on any operating system or cloud platform that supports Docker.\\n* This flexibility allows developers to deploy applications on their preferred infrastructure without worrying about compatibility issues.\\n\\n**6. Reduced Resource Consumption:**\\n* Unlike virtual machines, containers share the host system's kernel, which reduces memory and CPU overhead.\\n* This makes dockerized applications more efficient and cost-effective.\\n\\n**Additional Benefits:**\\n\\n* **Development and Testing:** Docker provides a consistent environment for development and testing, streamlining the workflow.\\n* **Continuous Integration/Continuous Deployment (CI/CD):** Docker integrates seamlessly with CI/CD pipelines, automating deployment and reducing release time.\\n* **Microservices Architecture:** Docker is ideal for deploying microservices-based applications, where each service runs in its own container.\", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 11, 'candidates_token_count': 417, 'total_token_count': 428}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is docker and how is it useful for deployment?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfNQP9jbhYCW"
   },
   "source": [
    "# 2.0 Create your first chain with LCEL\n",
    "\n",
    "LCEL is now the default way to create chains in LangChain. It has a more pipeline-like syntax and allows you to modify already-existing chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sc8OnZCxYvbT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_input = \"You are an English-French translator that return whatever the user says in French\" \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", prompt_input)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "26-oW1y_Z6hJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZ_tWUEVZ-uA",
    "outputId": "45493a04-fbdb-4179-9c63-1cfba4af05dc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Tu es un traducteur anglais-français qui renvoie tout ce que l'utilisateur dit en français\", response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 21, 'total_token_count': 36}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"input\": \"i enjoy going to rock concerts\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "T_xGLWivbeOf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add output parser to the chain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Wr5pD5_-ccrY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OBT74zgTch-D",
    "outputId": "5522a730-ffc7-4ab4-c422-e70f46303b06",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vous êtes un traducteur anglais-français qui renvoie tout ce que l'utilisateur dit en français.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"my friend robert has a blue cat\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8YDHxCZANdP",
    "outputId": "e23e6620-b58b-435f-9e88-98ecaa5f2e4f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**New Features**\\n\\n* **Model support:**\\n    * Added support for multiple models.\\n    * **CLIP**: A powerful image-text model from OpenAI.\\n    * **GPT-J**: A large language model from EleutherAI.\\n    * **T5**: A text-to-text transfer transformer model from Google AI.\\n    * **BART**: A sequence-to-sequence model from Facebook AI Research.\\n    * **mBART**: A multilingual sequence-to-sequence model from Facebook AI Research.\\n* **Chain support:**\\n    * **Chainable models**: Models can now be chained together to create complex language processing pipelines.\\n    * **Dynamic chain assembly**: Chains can be assembled dynamically at runtime.\\n* **Input and output handling:**\\n    * **Unified input representation**: Inputs are now represented as a single `InputSequence` object.\\n    * **Unified output representation**: Outputs are now represented as a single `OutputSequence` object.\\n    * **Streamlined input/output conversion**: Easy conversion between different input/output formats.\\n* **Interactive mode:**\\n    * **REPL-like interface**: A command-line interface for exploring models and chains interactively.\\n    * **Step-by-step execution**: Chains can be executed step-by-step to debug errors and visualize intermediate results.\\n\\n**Improvements**\\n\\n* **Enhanced documentation:** Comprehensive documentation is now available in both HTML and PDF formats.\\n* **Improved error handling:** More informative error messages and improved handling of model-specific errors.\\n* **Increased flexibility:**\\n    * **Custom chain builder**: Users can now define their own chain builder functions for increased flexibility.\\n    * **Chaining from scratch**: Support for chaining models without using predefined chains.\\n* **Performance optimizations:**\\n    * **Caching of model weights**: Reduces loading time for subsequent executions.\\n    * **Lazy execution**: Avoids unnecessary computations until needed.\\n\\n**Bug Fixes**\\n\\n* **Fixed:** Memory leak when using multiple models.\\n* **Fixed:** Occasional segmentation fault when chaining certain models.\\n* **Fixed:** Incorrect handling of Unicode characters in some models.', response_metadata={'is_blocked': False, 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability_label': 'NEGLIGIBLE', 'blocked': False}], 'citation_metadata': None, 'usage_metadata': {'prompt_token_count': 13, 'candidates_token_count': 447, 'total_token_count': 460}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is new in langchain 0.1.0?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPF1D7XzhoN-"
   },
   "source": [
    "# 3.0 Create a Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HziCjCdGj4X3"
   },
   "source": [
    "\n",
    "## 3.1 Load the source documents\n",
    "\n",
    "First, we will have to load the documents that will enrich our LLM prompt. We will use [this blog post](https://blog.langchain.dev/langchain-v0-1-0/) from LangChain's official website explaining the new release. OpenAI's models were not trained on this content, so the only way to ask questions about it is to build a RAG chain.\n",
    "\n",
    "The first thing to do is to load the blog content to our vector store. We will use beautiful soup to scrap the blog post. Then we will store it in a FAISS vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ndg7ytihAs2r",
    "outputId": "5e18451c-af5d-48a6-9125-735c57fbb998",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# retrieval chain\n",
    "\n",
    "% pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H69g1ttYA47U",
    "outputId": "83dda3a1-ca3b-4427-ad14-ca4ad8740d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "# ! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJBh0nZpA_jH"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://blog.langchain.dev/langchain-v0-1-0/\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-CjL7LPCZDa",
    "outputId": "f649b58b-6b85-4d64-9104-ac3c5f9554ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\nLangChain v0.1.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain v0.1.0\\n\\n10 min read\\nJan 8, 2024\\n\\n\\n\\n\\n\\nToday we‚Äôre excited to announce the release of langchain 0.1.0, our first stable version. It is fully backwards compatible, comes in both Python and JavaScript, and comes with improved focus through both functionality and documentation. A stable version of LangChain helps us earn developer trust and gives us the ability to evolve the library systematically and safely.Python GitHub DiscussionPython v0.1.0 GuidesJS v0.1.0 GuidesYouTube WalkthroughIntroductionLangChain has been around for a little over a year and has changed a lot as it‚Äôs grown to become the default framework for building LLM applications. As we previewed a month ago, we recently decided to make significant changes to the\\xa0 LangChain package architecture in order to better organize the project and strengthen the foundation.\\xa0Specifically we made two large architectural changes: separating out langchain-core and separating out partner packages (either into langchain-community or standalone partner packages) from langchain.\\xa0As a reminder, langchain-core contains the main abstractions, interfaces, and core functionality. This code is stable and has been following a stricter versioning policy for a little over a month now.langchain itself, however, still remained on 0.0.x versions. Having all releases on minor version 0 created a few challenges:Users couldn‚Äôt be confident that updating would not have breaking changeslangchain became bloated and unstable as we took a ‚Äúmaintain everything‚Äù approach to reduce breaking changes and deprecation notificationsHowever, starting today with the release of langchain 0.1.0, all future releases will follow a new versioning standard. Specifically:Any breaking changes to the public API will result in a minor version bump (the second digit)Any bug fixes or new features will result in a patch version bump (the third digit)We hope that this, combined with the previous architectural changes, will:Communicate clearly if breaking changes are made, allowing developers to update with confidenceGive us an avenue for officially deprecating and deleting old code, reducing bloatMore responsibly deal with integrations (whose SDKs are often changing as rapidly as LangChain)Even after we release a 0.2 version, we will commit to maintaining a branch of 0.1, but will only patch critical bug fixes. See more towards the end of this post on our plans for that.While re-architecting the package towards a path to a stable 0.1 release, we took the opportunity to talk to hundreds of developers about why they use LangChain and what they love about it. This input guided our direction and focus. We also used it as an opportunity to bring parity to the Python and JavaScript versions in the core areas outlined below. \\uf8ffüí°While certain integrations and more tangential chains may be language specific, core abstractions and key functionality are implemented equally in both the Python and JavaScript packages.We want to share what we‚Äôve heard and our plan to continually improve LangChain. We hope that sharing these learnings will increase transparency into our thinking and decisions, allowing others to better use, understand, and contribute to LangChain. After all, a huge part of LangChain is our community ‚Äì both the user base and the 2000+ contributors ‚Äì and we want everyone to come along for the journey.\\xa0Third Party IntegrationsOne of the things that people most love about LangChain is how easy we make it to get started building on any stack. We have almost 700 integrations, ranging from LLMs to vector stores to tools for agents to use. \\uf8ffüí°LangChain is often used as the ‚Äúglue‚Äù to join all the different pieces you need to build an LLM app together, and so prioritizing a robust integration ecosystem is a priority for us.About a month ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into langchain-community ‚Äì this allows us to centralize integration-specific work. We have also begun to split out individual integrations into their own packages. So far we have done this for ~10 packages, including OpenAI, Google and Mistral. One benefit of this is better dependency management - previously, all dependencies were optional, leading to some headaches when trying to install specific versions. Now if integrations are in their own package, we can more strictly version their requirements, leading to easier installation. Another benefit is versioning. Oftentimes, there are changes to the third party integrations, which require breaking changes. These can now be reflected on an individual integration basis with proper versioning in the standalone integration package.ObservabilityBuilding LLM applications involves putting a non-deterministic component at the center of your system. These models can often output unexpected results, so having visibility into exactly what is happening in your system is integral. \\uf8ffüí°We want to make langchain as observable and as debuggable as possible, whether through architectural decisions or tools we build on the side.We‚Äôve set about this in a few ways.The main way we‚Äôve tackled this is by building LangSmith. One of the main value props that LangSmith provides is a best-in-class debugging experience for your LLM application. We log exactly what steps are happening, what the inputs of each step are, what the outputs of each step are, how long each step takes, and more data. We display this in a user-friendly way, allowing you to identify which steps are taking the longest, enter a playground to debug unexpected LLM responses, track token usage and more. Even in private beta, the demand for LangSmith has been overwhelming, and we‚Äôre investing a lot in scalability so that we can release a public beta and then make it generally available in the coming months. We are also already supporting an enterprise version, which comes with a within-VPC deployment for enterprises with strict data privacy policies.We‚Äôve also tackled observability in other ways. We‚Äôve long had built in verbose and debug modes for different levels of logging throughout the pipeline. We recently introduced methods to visualize the chain you created, as well as get all prompts used.ComposabilityWhile it‚Äôs helpful to have prebuilt chains to get started, we very often see teams breaking outside of those architectures and wanting to customize their chain - not only customize the prompt, but also customize different parts of the orchestration. \\uf8ffüí°Over the past few months, we‚Äôve invested heavily in LangChain Expression Language (LCEL). This enables composition of arbitrary sequences, providing a lot of the same benefits as data orchestration tools do for data engineering pipelines (batching, parallelization, fallbacks). It also provides some benefits unique to LLM workloads - mainly LLM-specific observability (covered above), and streaming, covered later in this post.The components for LCEL are in langchain-core. We‚Äôve started to create higher level entry points for specific chains in LangChain. These will gradually replace pre-existing (now ‚ÄúLegacy‚Äù) chains, because chains built with LCEL will get streaming, ease of customization, observability, batching, retries out-of-the-box. Our goal is to make this transition seamless. Previously you may have done:ConversationalRetrievalChain.from_llm(llm, ‚Ä¶)We want to simply make it:create_conversational_retrieval_chain(llm, ‚Ä¶)Under the hood, it will create a specific LCEL chain and return it. If you want to modify the logic - no problem! Because it‚Äôs all written in LCEL it‚Äôs easy to modify part of it without having to subclass anything or override any methods.There are a lot of chains in LangChain, and a lot of them are heavily used. We will not deprecate the legacy version of the chain until an alternative constructor function exists and has been used and well-tested.StreamingLLMs can sometimes take a while to respond. It is important to show the end user that work is being done instead of staring at a blank screen. This can either be in the form of streaming tokens from the LLM or streaming intermediate steps (if a chain or agent is more long running).We‚Äôve invested heavily in both. All chains constructed with LCEL expose standard stream and astream methods, and we‚Äôve done a lot of work to ensure streaming outside of just the LLM call (for example, in output parsers). All chains also expose a standard astream_log method which streams all steps in the LCEL chain. These can then be filtered to easily get intermediate steps taken and other information.\\uf8ffüí°Streaming (of tokens and intermediate steps) is a crucial UX component for most LLM applications, and you get that for free with LangChain.Output ParsingOne of the main use cases for LangChain is ‚ÄúTool Usage‚Äù - using LLMs to invoke other tools.\\uf8ffüí°Making sure that an LLM returns information in a structured format that can be used in downstream applications is crucial for enabling LLMs to take actions.We‚Äôve invested a lot in a good developer experience around this, with the concept of output parsers.One of the main ways to do this is with OpenAI Function calling. We‚Äôve made it easy to not only specify the output format (using Pydantic, JSON schema, or even a function) but also to easily work with the response. We also support several different encoding methods (JSON, XML, Yaml) for when you want to do this with a model that doesn‚Äôt support OpenAI Function calling and resort to using prompting. When you resort to using prompting, you also need proper instructions to tell the LLM how to respond ‚Äì all output parsers come equipped with a get_format_instructions method to get those instructions.We‚Äôve also invested in more advanced functionality around output parsers, like allowing them to stream through partial results as they are generated to improve user experience. This includes streaming partial results from structured formats like JSON, XML and CSV.\\xa0 With output parsing, that can sometimes be tricky - in order to parse a JSON blob, most JSON parsers require a full JSON blob. A lot of our output parsers contain built-in logic to do this partial parsing.RetrievalOne of the main types of applications we see developers building is applications that interact with their private data. \\uf8ffüí°Being able to easily combine your data with LLMs is an incredibly important part of LangChain. This generally involves two different components - ingestion (preparing the data) and retrieval (retrieving the data), both of which we‚Äôve built out.On the ingestion side, a big part of ingestion is splitting the text you are working with into chunks. While this may seem trivial, the best way to do so is often nuanced and often specific to the type of document you are working with. We have 15 different text splitters, some optimized for specific document types (like HTML and Markdown) to give developers maximal control over this process. Relevant data often is changing though, and our ingestion system is designed for production, scaled applications. We‚Äôve exposed an indexing API to allow you to re-ingest content while ignoring pieces that have NOT changed - saving on time and cost for large-volume workloads.On the retrieval side, we‚Äôve invested in more advanced methods while also making retrieval more production ready. We‚Äôve implemented advanced retrieval strategies from academia (like FLARE and Hyde), created our own (like Parent Document and Self-Query), and adapted some from other industry solutions (like Multi-Query - derived from query expansion commonly used in search). We‚Äôve also made sure to support production concerns like per-user retrieval - crucial for any application where you are storing documents for multiple users together.Importantly, while LangChain provides all the necessary components for building advanced retrieval systems, we are not overly opinionated on how to do so. This has led to many other libraries building on top of LangChain to provide a more opinionated approach to retrieval - like EmbedChain and GPTResearcher.AgentsOne of the earliest things that LangChain became known for was agentic workloads. This can mean two things:Tool use: having an LLM call a function or toolReasoning: how to best enable an LLM to call a tool multiple times, and in what order (or not call a tool at all!)On the tool use side, we‚Äôve largely covered the components we see as crucial:Integrations with a large number of third party toolsWays to structure the LLMs response to fit the input schema of those toolsA flexible way to specify custom manners in which those tools should be invoked (LCEL)On the reasoning side, we have a few different ‚ÄúAgent‚Äù methods, which can largely be thought of as an LLM running in a loop, deciding each iteration which (if any) tool it needs to call and then observing the result of that tool. We incorporated ReAct (an early prompting strategy for doing so) from the beginning, and have quickly added in many other types, including ones that use OpenAI Function Calling, ones that use their new tool-calling API, ones optimized for conversation, and more.\\uf8ffüí°Through flexible and extensible tool support and advanced reasoning capabilities, LangChain has the become the default way to enable LLMs to take actions.Similar to retrieval, while LangChain provides the building blocks for agents we\\'ve also seen several more opinionated frameworks built on top. A great example of this is CrewAI, which builds on top of LangChain to provide an easier interface for multi-agent workloads.LangChain 0.2Even though we just released LangChain 0.1, we‚Äôre already thinking about 0.2. Some things that are top of mind for us are:Rewriting legacy chains in LCEL (with better streaming and debugging support)Adding new types of chainsAdding new types of agentsImproving our production ingestion capabilitiesRemoving old and unused functionalityImportantly, even though we are excited about removing some of the old and legacy code to make langchain slimmer and more focused, we also want to maintain support for people who are still using the old version. That is why we will maintain 0.1 as a stable branch (patching in critical bug fixes) for at least 3 months after 0.2 release. We plan to do this for every stable release from here on out.And if you\\'ve been wanting to get started contributing, there\\'s never been a better time. We recently added a good getting started issue on GitHub if you\\'re looking for a place to start.One More ThingA large part of LangChain v0.1.0 is stability and focus on the core areas outlined above. Now that we\\'ve identified the areas people love about LangChain, we can work on adding more advanced and complete tooling there.One of the main things people love about LangChain is it\\'s support for agents. Most agents are largely defined as running an LLM in some sort of a loop. So far, the only way we\\'ve had to do that is with AgentExecutor. We\\'ve added a lot of parameters and functionality to AgentExecutor, but its still just one way of running a loop.\\uf8ffüí°We\\'re excited to announce the release of langgraph, a new library to allow for creating language agents as graphs.This will allow users to create far more custom cyclical behavior. You can define explicit planning steps, explicit reflection steps, or easily hard code it so that a specific tool is always called first.It is inspired by\\xa0Pregel\\xa0and\\xa0Apache Beam. The current interface exposed is one inspired by\\xa0NetworkX, and looks something like:from langgraph.graph import END, Graph\\n\\nworkflow = Graph()\\n\\nworkflow.add_node(\"agent\", agent)\\nworkflow.add_node(\"tools\", execute_tools)\\n\\nworkflow.set_entry_point(\"agent\")\\n\\nworkflow.add_conditional_edges(\\n    \"agent\",\\n    should_continue,\\n    {\\n        \"continue\": \"tools\",\\n        \"exit\": END\\n    }\\n)\\n\\nworkflow.add_edge(\\'tools\\', \\'agent\\')\\n\\nchain = workflow.compile()We\\'ve been working on this for the past six months, beta-testing it with users. It currently powers OpenGPTs. We\\'ll be adding a lot more examples and documentation over the next few weeks - we\\'re really excited about this!Try it out here.ConclusionLangChain has evolved significantly along with the ecosystem. We are incredibly grateful to our community and users for pushing us and building with us. With this 0.1 release, we‚Äôve taken time to understand what you want and need in an LLM framework, and remain committed to building it. As the community‚Äôs needs evolve (or if we‚Äôre missing something), we want to hear your feedback, so we can address it. They say, ‚ÄúA journey of a thousand miles begins with a single step.‚Äù ‚Äì or in our case, version 0.1.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            ¬© LangChain Blog 2024 - Powered by Ghost\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uc3aDflhCZwk"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQER2E4bCwp1"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter  import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khPB0xuDDM8m",
    "outputId": "f7590ed8-7f24-451e-c255-309f012dfb9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangChain v0.1.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain v0.1.0\\n\\n10 min read\\nJan 8, 2024', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content='Today we‚Äôre excited to announce the release of langchain 0.1.0, our first stable version. It is fully backwards compatible, comes in both Python and JavaScript, and comes with improved focus through both functionality and documentation. A stable version of LangChain helps us earn developer trust and gives us the ability to evolve the library systematically and safely.Python GitHub DiscussionPython v0.1.0 GuidesJS v0.1.0 GuidesYouTube WalkthroughIntroductionLangChain has been around for a little over a year and has changed a lot as it‚Äôs grown to become the default framework for building LLM applications. As we previewed a month ago, we recently decided to make significant changes to the\\xa0 LangChain package architecture in order to better organize the project and strengthen the foundation.\\xa0Specifically we made two large architectural changes: separating out langchain-core and separating out partner packages (either into langchain-community or standalone partner packages) from langchain.\\xa0As a reminder, langchain-core contains the main abstractions, interfaces, and core functionality. This code is stable and has been following a stricter versioning policy for a little over a month now.langchain itself, however, still remained on 0.0.x versions. Having all releases on minor version 0 created a few challenges:Users couldn‚Äôt be confident that updating would not have breaking changeslangchain became bloated and unstable as we took a ‚Äúmaintain everything‚Äù approach to reduce breaking changes and deprecation notificationsHowever, starting today with the release of langchain 0.1.0, all future releases will follow a new versioning standard. Specifically:Any breaking changes to the public API will result in a minor version bump (the second digit)Any bug fixes or new features will result in a patch version bump (the third digit)We hope that this, combined with the previous architectural changes, will:Communicate clearly if breaking changes are made, allowing developers to update with confidenceGive us an avenue for officially deprecating and deleting old code, reducing bloatMore responsibly deal with integrations (whose SDKs are often changing as rapidly as LangChain)Even after we release a 0.2 version, we will commit to maintaining a branch of 0.1, but will only patch critical bug fixes. See more towards the end of this post on our plans for that.While re-architecting the package towards a path to a stable 0.1 release, we took the opportunity to talk to hundreds of developers about why they use LangChain and what they love about it. This input guided our direction and focus. We also used it as an opportunity to bring parity to the Python and JavaScript versions in the core areas outlined below. \\uf8ffüí°While certain integrations and more tangential chains may be language specific, core abstractions and key functionality are implemented equally in both the Python and JavaScript packages.We want to share what we‚Äôve heard and our plan to continually improve LangChain. We hope that sharing these learnings will increase transparency into our thinking and decisions, allowing others to better use, understand, and contribute to LangChain. After all, a huge part of LangChain is our community ‚Äì both the user base and the 2000+ contributors ‚Äì and we want everyone to come along for the journey.\\xa0Third Party IntegrationsOne of the things that people most love about LangChain is how easy we make it to get started building on any stack. We have almost 700 integrations, ranging from LLMs to vector stores to tools for agents to use. \\uf8ffüí°LangChain is often used as the ‚Äúglue‚Äù to join all the different pieces you need to build an LLM app together, and so prioritizing a robust integration ecosystem is a priority for us.About a month ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content='ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into langchain-community ‚Äì this allows us to centralize integration-specific work. We have also begun to split out individual integrations into their own packages. So far we have done this for ~10 packages, including OpenAI, Google and Mistral. One benefit of this is better dependency management - previously, all dependencies were optional, leading to some headaches when trying to install specific versions. Now if integrations are in their own package, we can more strictly version their requirements, leading to easier installation. Another benefit is versioning. Oftentimes, there are changes to the third party integrations, which require breaking changes. These can now be reflected on an individual integration basis with proper versioning in the standalone integration package.ObservabilityBuilding LLM applications involves putting a non-deterministic component at the center of your system. These models can often output unexpected results, so having visibility into exactly what is happening in your system is integral. \\uf8ffüí°We want to make langchain as observable and as debuggable as possible, whether through architectural decisions or tools we build on the side.We‚Äôve set about this in a few ways.The main way we‚Äôve tackled this is by building LangSmith. One of the main value props that LangSmith provides is a best-in-class debugging experience for your LLM application. We log exactly what steps are happening, what the inputs of each step are, what the outputs of each step are, how long each step takes, and more data. We display this in a user-friendly way, allowing you to identify which steps are taking the longest, enter a playground to debug unexpected LLM responses, track token usage and more. Even in private beta, the demand for LangSmith has been overwhelming, and we‚Äôre investing a lot in scalability so that we can release a public beta and then make it generally available in the coming months. We are also already supporting an enterprise version, which comes with a within-VPC deployment for enterprises with strict data privacy policies.We‚Äôve also tackled observability in other ways. We‚Äôve long had built in verbose and debug modes for different levels of logging throughout the pipeline. We recently introduced methods to visualize the chain you created, as well as get all prompts used.ComposabilityWhile it‚Äôs helpful to have prebuilt chains to get started, we very often see teams breaking outside of those architectures and wanting to customize their chain - not only customize the prompt, but also customize different parts of the orchestration. \\uf8ffüí°Over the past few months, we‚Äôve invested heavily in LangChain Expression Language (LCEL). This enables composition of arbitrary sequences, providing a lot of the same benefits as data orchestration tools do for data engineering pipelines (batching, parallelization, fallbacks). It also provides some benefits unique to LLM workloads - mainly LLM-specific observability (covered above), and streaming, covered later in this post.The components for LCEL are in langchain-core. We‚Äôve started to create higher level entry points for specific chains in LangChain. These will gradually replace pre-existing (now ‚ÄúLegacy‚Äù) chains, because chains built with LCEL will get streaming, ease of customization, observability, batching, retries out-of-the-box. Our goal is to make this transition seamless. Previously you may have done:ConversationalRetrievalChain.from_llm(llm, ‚Ä¶)We want to simply make it:create_conversational_retrieval_chain(llm, ‚Ä¶)Under the hood, it will create a specific LCEL chain and return it. If you want to modify the logic - no problem! Because it‚Äôs all written in LCEL it‚Äôs easy to modify part of it without having to subclass anything or override any methods.There', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content='and return it. If you want to modify the logic - no problem! Because it‚Äôs all written in LCEL it‚Äôs easy to modify part of it without having to subclass anything or override any methods.There are a lot of chains in LangChain, and a lot of them are heavily used. We will not deprecate the legacy version of the chain until an alternative constructor function exists and has been used and well-tested.StreamingLLMs can sometimes take a while to respond. It is important to show the end user that work is being done instead of staring at a blank screen. This can either be in the form of streaming tokens from the LLM or streaming intermediate steps (if a chain or agent is more long running).We‚Äôve invested heavily in both. All chains constructed with LCEL expose standard stream and astream methods, and we‚Äôve done a lot of work to ensure streaming outside of just the LLM call (for example, in output parsers). All chains also expose a standard astream_log method which streams all steps in the LCEL chain. These can then be filtered to easily get intermediate steps taken and other information.\\uf8ffüí°Streaming (of tokens and intermediate steps) is a crucial UX component for most LLM applications, and you get that for free with LangChain.Output ParsingOne of the main use cases for LangChain is ‚ÄúTool Usage‚Äù - using LLMs to invoke other tools.\\uf8ffüí°Making sure that an LLM returns information in a structured format that can be used in downstream applications is crucial for enabling LLMs to take actions.We‚Äôve invested a lot in a good developer experience around this, with the concept of output parsers.One of the main ways to do this is with OpenAI Function calling. We‚Äôve made it easy to not only specify the output format (using Pydantic, JSON schema, or even a function) but also to easily work with the response. We also support several different encoding methods (JSON, XML, Yaml) for when you want to do this with a model that doesn‚Äôt support OpenAI Function calling and resort to using prompting. When you resort to using prompting, you also need proper instructions to tell the LLM how to respond ‚Äì all output parsers come equipped with a get_format_instructions method to get those instructions.We‚Äôve also invested in more advanced functionality around output parsers, like allowing them to stream through partial results as they are generated to improve user experience. This includes streaming partial results from structured formats like JSON, XML and CSV.\\xa0 With output parsing, that can sometimes be tricky - in order to parse a JSON blob, most JSON parsers require a full JSON blob. A lot of our output parsers contain built-in logic to do this partial parsing.RetrievalOne of the main types of applications we see developers building is applications that interact with their private data. \\uf8ffüí°Being able to easily combine your data with LLMs is an incredibly important part of LangChain. This generally involves two different components - ingestion (preparing the data) and retrieval (retrieving the data), both of which we‚Äôve built out.On the ingestion side, a big part of ingestion is splitting the text you are working with into chunks. While this may seem trivial, the best way to do so is often nuanced and often specific to the type of document you are working with. We have 15 different text splitters, some optimized for specific document types (like HTML and Markdown) to give developers maximal control over this process. Relevant data often is changing though, and our ingestion system is designed for production, scaled applications. We‚Äôve exposed an indexing API to allow you to re-ingest content while ignoring pieces that have NOT changed - saving on time and cost for large-volume workloads.On the retrieval side, we‚Äôve invested in more advanced methods while also making retrieval more production ready. We‚Äôve implemented advanced retrieval strategies from academia (like FLARE and Hyde), created our own (like Parent Document and', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content=\"advanced methods while also making retrieval more production ready. We‚Äôve implemented advanced retrieval strategies from academia (like FLARE and Hyde), created our own (like Parent Document and Self-Query), and adapted some from other industry solutions (like Multi-Query - derived from query expansion commonly used in search). We‚Äôve also made sure to support production concerns like per-user retrieval - crucial for any application where you are storing documents for multiple users together.Importantly, while LangChain provides all the necessary components for building advanced retrieval systems, we are not overly opinionated on how to do so. This has led to many other libraries building on top of LangChain to provide a more opinionated approach to retrieval - like EmbedChain and GPTResearcher.AgentsOne of the earliest things that LangChain became known for was agentic workloads. This can mean two things:Tool use: having an LLM call a function or toolReasoning: how to best enable an LLM to call a tool multiple times, and in what order (or not call a tool at all!)On the tool use side, we‚Äôve largely covered the components we see as crucial:Integrations with a large number of third party toolsWays to structure the LLMs response to fit the input schema of those toolsA flexible way to specify custom manners in which those tools should be invoked (LCEL)On the reasoning side, we have a few different ‚ÄúAgent‚Äù methods, which can largely be thought of as an LLM running in a loop, deciding each iteration which (if any) tool it needs to call and then observing the result of that tool. We incorporated ReAct (an early prompting strategy for doing so) from the beginning, and have quickly added in many other types, including ones that use OpenAI Function Calling, ones that use their new tool-calling API, ones optimized for conversation, and more.\\uf8ffüí°Through flexible and extensible tool support and advanced reasoning capabilities, LangChain has the become the default way to enable LLMs to take actions.Similar to retrieval, while LangChain provides the building blocks for agents we've also seen several more opinionated frameworks built on top. A great example of this is CrewAI, which builds on top of LangChain to provide an easier interface for multi-agent workloads.LangChain 0.2Even though we just released LangChain 0.1, we‚Äôre already thinking about 0.2. Some things that are top of mind for us are:Rewriting legacy chains in LCEL (with better streaming and debugging support)Adding new types of chainsAdding new types of agentsImproving our production ingestion capabilitiesRemoving old and unused functionalityImportantly, even though we are excited about removing some of the old and legacy code to make langchain slimmer and more focused, we also want to maintain support for people who are still using the old version. That is why we will maintain 0.1 as a stable branch (patching in critical bug fixes) for at least 3 months after 0.2 release. We plan to do this for every stable release from here on out.And if you've been wanting to get started contributing, there's never been a better time. We recently added a good getting started issue on GitHub if you're looking for a place to start.One More ThingA large part of LangChain v0.1.0 is stability and focus on the core areas outlined above. Now that we've identified the areas people love about LangChain, we can work on adding more advanced and complete tooling there.One of the main things people love about LangChain is it's support for agents. Most agents are largely defined as running an LLM in some sort of a loop. So far, the only way we've had to do that is with AgentExecutor. We've added a lot of parameters and functionality to AgentExecutor, but its still just one way of running a loop.\\uf8ffüí°We're excited to announce the release of langgraph, a new library to allow for creating language agents as graphs.This will allow users to create far more custom cyclical behavior. You can define\", metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content='excited to announce the release of langgraph, a new library to allow for creating language agents as graphs.This will allow users to create far more custom cyclical behavior. You can define explicit planning steps, explicit reflection steps, or easily hard code it so that a specific tool is always called first.It is inspired by\\xa0Pregel\\xa0and\\xa0Apache Beam. The current interface exposed is one inspired by\\xa0NetworkX, and looks something like:from langgraph.graph import END, Graph', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content='workflow = Graph()\\n\\nworkflow.add_node(\"agent\", agent)\\nworkflow.add_node(\"tools\", execute_tools)\\n\\nworkflow.set_entry_point(\"agent\")\\n\\nworkflow.add_conditional_edges(\\n    \"agent\",\\n    should_continue,\\n    {\\n        \"continue\": \"tools\",\\n        \"exit\": END\\n    }\\n)\\n\\nworkflow.add_edge(\\'tools\\', \\'agent\\')\\n\\nchain = workflow.compile()We\\'ve been working on this for the past six months, beta-testing it with users. It currently powers OpenGPTs. We\\'ll be adding a lot more examples and documentation over the next few weeks - we\\'re really excited about this!Try it out here.ConclusionLangChain has evolved significantly along with the ecosystem. We are incredibly grateful to our community and users for pushing us and building with us. With this 0.1 release, we‚Äôve taken time to understand what you want and need in an LLM framework, and remain committed to building it. As the community‚Äôs needs evolve (or if we‚Äôre missing something), we want to hear your feedback, so we can address it. They say, ‚ÄúA journey of a thousand miles begins with a single step.‚Äù ‚Äì or in our case, version 0.1.\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            ¬© LangChain Blog 2024 - Powered by Ghost', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W7kj0RoDNzV"
   },
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtWWJQzhiqHl"
   },
   "source": [
    "## 3.2 Create a Context-Aware LLM Chain\n",
    "\n",
    "Here we create a chain that will answer a question given a context. For now, we are passing the context manually, but in the next step we will pass in the documents fetched from the vector store we created above 👆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrf_EX26DRHO"
   },
   "outputs": [],
   "source": [
    "# create chain for documents\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "template = \"\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2caJnGH-FFV0",
    "outputId": "8552a967-5843-47a3-a96f-1f7d6c43e7fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Langchain 0.1.0 is the new version of a llm app development framework.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"what is langchain 0.1.0?\",\n",
    "    \"context\": [Document(page_content=\"langchain 0.1.0 is the new version of a llm app development framework.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oW_TGIBjj_c"
   },
   "source": [
    "## 3.2 Create the RAG Chain\n",
    "\n",
    "RAG stands for Retrieval-Augmented Generation. This means that we will enrich the prompt that we send to the LLM. We will use with the documents that wil will retrieve from the vector store for this. LangChain comes with the function `create_retrieval_chain` that allows you to create one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhvXHSkPHb0K"
   },
   "outputs": [],
   "source": [
    "# create retrieval chain\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtdL4bsEFrP_"
   },
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"what is new in langchain 0.1.0\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "NvL_yXp-I-AG",
    "outputId": "23d86a2a-b38d-45c8-d816-018993707a11"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'In langchain 0.1.0, the new features include improved focus through both functionality and documentation, a new versioning standard, the separation of langchain-core and partner packages, and the introduction of LangSmith for debugging LLM applications.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKELJ-LVkE07"
   },
   "source": [
    "# 4.0 Create Conversational RAG Chain\n",
    "\n",
    "Now we will create exactly the same thing as above, but we will have the AI assistant take the history of the conversation into account. In short, we will build the same chain as above but with we will take into account the previous messages in these two steps of the chain:\n",
    "\n",
    "- When fetching the documents from the vector store. We will fetch documents related to the entire conversation and not just the latest message.\n",
    "- When answering the question. We will send to the LLM the history of the conversation along the context and query.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHkQopzYqlaC"
   },
   "source": [
    "## 4.1 Create a Conversation-Aware Retrieval Chain\n",
    "\n",
    "This chain will return the documents related to the entire conversation and not just the latest message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZmG7GY4I_XG"
   },
   "outputs": [],
   "source": [
    "# conversational retrieval chain\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6zrgY-KSys4",
    "outputId": "5edefc84-6fb4-4b84-c8a2-102a615f5615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Today we‚Äôre excited to announce the release of langchain 0.1.0, our first stable version. It is fully backwards compatible, comes in both Python and JavaScript, and comes with improved focus through both functionality and documentation. A stable version of LangChain helps us earn developer trust and gives us the ability to evolve the library systematically and safely.Python GitHub DiscussionPython v0.1.0 GuidesJS v0.1.0 GuidesYouTube WalkthroughIntroductionLangChain has been around for a little over a year and has changed a lot as it‚Äôs grown to become the default framework for building LLM applications. As we previewed a month ago, we recently decided to make significant changes to the\\xa0 LangChain package architecture in order to better organize the project and strengthen the foundation.\\xa0Specifically we made two large architectural changes: separating out langchain-core and separating out partner packages (either into langchain-community or standalone partner packages) from langchain.\\xa0As a reminder, langchain-core contains the main abstractions, interfaces, and core functionality. This code is stable and has been following a stricter versioning policy for a little over a month now.langchain itself, however, still remained on 0.0.x versions. Having all releases on minor version 0 created a few challenges:Users couldn‚Äôt be confident that updating would not have breaking changeslangchain became bloated and unstable as we took a ‚Äúmaintain everything‚Äù approach to reduce breaking changes and deprecation notificationsHowever, starting today with the release of langchain 0.1.0, all future releases will follow a new versioning standard. Specifically:Any breaking changes to the public API will result in a minor version bump (the second digit)Any bug fixes or new features will result in a patch version bump (the third digit)We hope that this, combined with the previous architectural changes, will:Communicate clearly if breaking changes are made, allowing developers to update with confidenceGive us an avenue for officially deprecating and deleting old code, reducing bloatMore responsibly deal with integrations (whose SDKs are often changing as rapidly as LangChain)Even after we release a 0.2 version, we will commit to maintaining a branch of 0.1, but will only patch critical bug fixes. See more towards the end of this post on our plans for that.While re-architecting the package towards a path to a stable 0.1 release, we took the opportunity to talk to hundreds of developers about why they use LangChain and what they love about it. This input guided our direction and focus. We also used it as an opportunity to bring parity to the Python and JavaScript versions in the core areas outlined below. \\uf8ffüí°While certain integrations and more tangential chains may be language specific, core abstractions and key functionality are implemented equally in both the Python and JavaScript packages.We want to share what we‚Äôve heard and our plan to continually improve LangChain. We hope that sharing these learnings will increase transparency into our thinking and decisions, allowing others to better use, understand, and contribute to LangChain. After all, a huge part of LangChain is our community ‚Äì both the user base and the 2000+ contributors ‚Äì and we want everyone to come along for the journey.\\xa0Third Party IntegrationsOne of the things that people most love about LangChain is how easy we make it to get started building on any stack. We have almost 700 integrations, ranging from LLMs to vector stores to tools for agents to use. \\uf8ffüí°LangChain is often used as the ‚Äúglue‚Äù to join all the different pieces you need to build an LLM app together, and so prioritizing a robust integration ecosystem is a priority for us.About a month ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content='LangChain v0.1.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain v0.1.0\\n\\n10 min read\\nJan 8, 2024', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content=\"advanced methods while also making retrieval more production ready. We‚Äôve implemented advanced retrieval strategies from academia (like FLARE and Hyde), created our own (like Parent Document and Self-Query), and adapted some from other industry solutions (like Multi-Query - derived from query expansion commonly used in search). We‚Äôve also made sure to support production concerns like per-user retrieval - crucial for any application where you are storing documents for multiple users together.Importantly, while LangChain provides all the necessary components for building advanced retrieval systems, we are not overly opinionated on how to do so. This has led to many other libraries building on top of LangChain to provide a more opinionated approach to retrieval - like EmbedChain and GPTResearcher.AgentsOne of the earliest things that LangChain became known for was agentic workloads. This can mean two things:Tool use: having an LLM call a function or toolReasoning: how to best enable an LLM to call a tool multiple times, and in what order (or not call a tool at all!)On the tool use side, we‚Äôve largely covered the components we see as crucial:Integrations with a large number of third party toolsWays to structure the LLMs response to fit the input schema of those toolsA flexible way to specify custom manners in which those tools should be invoked (LCEL)On the reasoning side, we have a few different ‚ÄúAgent‚Äù methods, which can largely be thought of as an LLM running in a loop, deciding each iteration which (if any) tool it needs to call and then observing the result of that tool. We incorporated ReAct (an early prompting strategy for doing so) from the beginning, and have quickly added in many other types, including ones that use OpenAI Function Calling, ones that use their new tool-calling API, ones optimized for conversation, and more.\\uf8ffüí°Through flexible and extensible tool support and advanced reasoning capabilities, LangChain has the become the default way to enable LLMs to take actions.Similar to retrieval, while LangChain provides the building blocks for agents we've also seen several more opinionated frameworks built on top. A great example of this is CrewAI, which builds on top of LangChain to provide an easier interface for multi-agent workloads.LangChain 0.2Even though we just released LangChain 0.1, we‚Äôre already thinking about 0.2. Some things that are top of mind for us are:Rewriting legacy chains in LCEL (with better streaming and debugging support)Adding new types of chainsAdding new types of agentsImproving our production ingestion capabilitiesRemoving old and unused functionalityImportantly, even though we are excited about removing some of the old and legacy code to make langchain slimmer and more focused, we also want to maintain support for people who are still using the old version. That is why we will maintain 0.1 as a stable branch (patching in critical bug fixes) for at least 3 months after 0.2 release. We plan to do this for every stable release from here on out.And if you've been wanting to get started contributing, there's never been a better time. We recently added a good getting started issue on GitHub if you're looking for a place to start.One More ThingA large part of LangChain v0.1.0 is stability and focus on the core areas outlined above. Now that we've identified the areas people love about LangChain, we can work on adding more advanced and complete tooling there.One of the main things people love about LangChain is it's support for agents. Most agents are largely defined as running an LLM in some sort of a loop. So far, the only way we've had to do that is with AgentExecutor. We've added a lot of parameters and functionality to AgentExecutor, but its still just one way of running a loop.\\uf8ffüí°We're excited to announce the release of langgraph, a new library to allow for creating language agents as graphs.This will allow users to create far more custom cyclical behavior. You can define\", metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       " Document(page_content='ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into langchain-community ‚Äì this allows us to centralize integration-specific work. We have also begun to split out individual integrations into their own packages. So far we have done this for ~10 packages, including OpenAI, Google and Mistral. One benefit of this is better dependency management - previously, all dependencies were optional, leading to some headaches when trying to install specific versions. Now if integrations are in their own package, we can more strictly version their requirements, leading to easier installation. Another benefit is versioning. Oftentimes, there are changes to the third party integrations, which require breaking changes. These can now be reflected on an individual integration basis with proper versioning in the standalone integration package.ObservabilityBuilding LLM applications involves putting a non-deterministic component at the center of your system. These models can often output unexpected results, so having visibility into exactly what is happening in your system is integral. \\uf8ffüí°We want to make langchain as observable and as debuggable as possible, whether through architectural decisions or tools we build on the side.We‚Äôve set about this in a few ways.The main way we‚Äôve tackled this is by building LangSmith. One of the main value props that LangSmith provides is a best-in-class debugging experience for your LLM application. We log exactly what steps are happening, what the inputs of each step are, what the outputs of each step are, how long each step takes, and more data. We display this in a user-friendly way, allowing you to identify which steps are taking the longest, enter a playground to debug unexpected LLM responses, track token usage and more. Even in private beta, the demand for LangSmith has been overwhelming, and we‚Äôre investing a lot in scalability so that we can release a public beta and then make it generally available in the coming months. We are also already supporting an enterprise version, which comes with a within-VPC deployment for enterprises with strict data privacy policies.We‚Äôve also tackled observability in other ways. We‚Äôve long had built in verbose and debug modes for different levels of logging throughout the pipeline. We recently introduced methods to visualize the chain you created, as well as get all prompts used.ComposabilityWhile it‚Äôs helpful to have prebuilt chains to get started, we very often see teams breaking outside of those architectures and wanting to customize their chain - not only customize the prompt, but also customize different parts of the orchestration. \\uf8ffüí°Over the past few months, we‚Äôve invested heavily in LangChain Expression Language (LCEL). This enables composition of arbitrary sequences, providing a lot of the same benefits as data orchestration tools do for data engineering pipelines (batching, parallelization, fallbacks). It also provides some benefits unique to LLM workloads - mainly LLM-specific observability (covered above), and streaming, covered later in this post.The components for LCEL are in langchain-core. We‚Äôve started to create higher level entry points for specific chains in LangChain. These will gradually replace pre-existing (now ‚ÄúLegacy‚Äù) chains, because chains built with LCEL will get streaming, ease of customization, observability, batching, retries out-of-the-box. Our goal is to make this transition seamless. Previously you may have done:ConversationalRetrievalChain.from_llm(llm, ‚Ä¶)We want to simply make it:create_conversational_retrieval_chain(llm, ‚Ä¶)Under the hood, it will create a specific LCEL chain and return it. If you want to modify the logic - no problem! Because it‚Äôs all written in LCEL it‚Äôs easy to modify part of it without having to subclass anything or override any methods.There', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Is there anything new about Langchain 0.1.0?\"),\n",
    "    AIMessage(content=\"Yes!\")\n",
    "]\n",
    "\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me more about it!\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNjXf9UNq3Wv"
   },
   "source": [
    "## 4.2 Use Retrieval Chain together with Document Chain\n",
    "\n",
    "Now we will create a document chain that contains a placeholder for the conversation history. This placeholder will be populated with the conversation history that we will pass as its value. We will the plug it together with the retriever chain we created above to have a conversational retrieval-augmented chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKOR41J8Vj6s"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "conversational_retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rm_KBfo6XxkJ"
   },
   "outputs": [],
   "source": [
    "response = conversational_retrieval_chain.invoke({\n",
    "    'chat_history': [],\n",
    "    \"input\": \"What is langchain 0.1.0 about?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOWw4-FLX-i0",
    "outputId": "f2a9a902-52b7-4ddc-d8a1-89516a56b1e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [],\n",
       " 'input': 'What is langchain 0.1.0 about?',\n",
       " 'context': [Document(page_content='Today we‚Äôre excited to announce the release of langchain 0.1.0, our first stable version. It is fully backwards compatible, comes in both Python and JavaScript, and comes with improved focus through both functionality and documentation. A stable version of LangChain helps us earn developer trust and gives us the ability to evolve the library systematically and safely.Python GitHub DiscussionPython v0.1.0 GuidesJS v0.1.0 GuidesYouTube WalkthroughIntroductionLangChain has been around for a little over a year and has changed a lot as it‚Äôs grown to become the default framework for building LLM applications. As we previewed a month ago, we recently decided to make significant changes to the\\xa0 LangChain package architecture in order to better organize the project and strengthen the foundation.\\xa0Specifically we made two large architectural changes: separating out langchain-core and separating out partner packages (either into langchain-community or standalone partner packages) from langchain.\\xa0As a reminder, langchain-core contains the main abstractions, interfaces, and core functionality. This code is stable and has been following a stricter versioning policy for a little over a month now.langchain itself, however, still remained on 0.0.x versions. Having all releases on minor version 0 created a few challenges:Users couldn‚Äôt be confident that updating would not have breaking changeslangchain became bloated and unstable as we took a ‚Äúmaintain everything‚Äù approach to reduce breaking changes and deprecation notificationsHowever, starting today with the release of langchain 0.1.0, all future releases will follow a new versioning standard. Specifically:Any breaking changes to the public API will result in a minor version bump (the second digit)Any bug fixes or new features will result in a patch version bump (the third digit)We hope that this, combined with the previous architectural changes, will:Communicate clearly if breaking changes are made, allowing developers to update with confidenceGive us an avenue for officially deprecating and deleting old code, reducing bloatMore responsibly deal with integrations (whose SDKs are often changing as rapidly as LangChain)Even after we release a 0.2 version, we will commit to maintaining a branch of 0.1, but will only patch critical bug fixes. See more towards the end of this post on our plans for that.While re-architecting the package towards a path to a stable 0.1 release, we took the opportunity to talk to hundreds of developers about why they use LangChain and what they love about it. This input guided our direction and focus. We also used it as an opportunity to bring parity to the Python and JavaScript versions in the core areas outlined below. \\uf8ffüí°While certain integrations and more tangential chains may be language specific, core abstractions and key functionality are implemented equally in both the Python and JavaScript packages.We want to share what we‚Äôve heard and our plan to continually improve LangChain. We hope that sharing these learnings will increase transparency into our thinking and decisions, allowing others to better use, understand, and contribute to LangChain. After all, a huge part of LangChain is our community ‚Äì both the user base and the 2000+ contributors ‚Äì and we want everyone to come along for the journey.\\xa0Third Party IntegrationsOne of the things that people most love about LangChain is how easy we make it to get started building on any stack. We have almost 700 integrations, ranging from LLMs to vector stores to tools for agents to use. \\uf8ffüí°LangChain is often used as the ‚Äúglue‚Äù to join all the different pieces you need to build an LLM app together, and so prioritizing a robust integration ecosystem is a priority for us.About a month ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       "  Document(page_content='LangChain v0.1.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain v0.1.0\\n\\n10 min read\\nJan 8, 2024', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       "  Document(page_content=\"advanced methods while also making retrieval more production ready. We‚Äôve implemented advanced retrieval strategies from academia (like FLARE and Hyde), created our own (like Parent Document and Self-Query), and adapted some from other industry solutions (like Multi-Query - derived from query expansion commonly used in search). We‚Äôve also made sure to support production concerns like per-user retrieval - crucial for any application where you are storing documents for multiple users together.Importantly, while LangChain provides all the necessary components for building advanced retrieval systems, we are not overly opinionated on how to do so. This has led to many other libraries building on top of LangChain to provide a more opinionated approach to retrieval - like EmbedChain and GPTResearcher.AgentsOne of the earliest things that LangChain became known for was agentic workloads. This can mean two things:Tool use: having an LLM call a function or toolReasoning: how to best enable an LLM to call a tool multiple times, and in what order (or not call a tool at all!)On the tool use side, we‚Äôve largely covered the components we see as crucial:Integrations with a large number of third party toolsWays to structure the LLMs response to fit the input schema of those toolsA flexible way to specify custom manners in which those tools should be invoked (LCEL)On the reasoning side, we have a few different ‚ÄúAgent‚Äù methods, which can largely be thought of as an LLM running in a loop, deciding each iteration which (if any) tool it needs to call and then observing the result of that tool. We incorporated ReAct (an early prompting strategy for doing so) from the beginning, and have quickly added in many other types, including ones that use OpenAI Function Calling, ones that use their new tool-calling API, ones optimized for conversation, and more.\\uf8ffüí°Through flexible and extensible tool support and advanced reasoning capabilities, LangChain has the become the default way to enable LLMs to take actions.Similar to retrieval, while LangChain provides the building blocks for agents we've also seen several more opinionated frameworks built on top. A great example of this is CrewAI, which builds on top of LangChain to provide an easier interface for multi-agent workloads.LangChain 0.2Even though we just released LangChain 0.1, we‚Äôre already thinking about 0.2. Some things that are top of mind for us are:Rewriting legacy chains in LCEL (with better streaming and debugging support)Adding new types of chainsAdding new types of agentsImproving our production ingestion capabilitiesRemoving old and unused functionalityImportantly, even though we are excited about removing some of the old and legacy code to make langchain slimmer and more focused, we also want to maintain support for people who are still using the old version. That is why we will maintain 0.1 as a stable branch (patching in critical bug fixes) for at least 3 months after 0.2 release. We plan to do this for every stable release from here on out.And if you've been wanting to get started contributing, there's never been a better time. We recently added a good getting started issue on GitHub if you're looking for a place to start.One More ThingA large part of LangChain v0.1.0 is stability and focus on the core areas outlined above. Now that we've identified the areas people love about LangChain, we can work on adding more advanced and complete tooling there.One of the main things people love about LangChain is it's support for agents. Most agents are largely defined as running an LLM in some sort of a loop. So far, the only way we've had to do that is with AgentExecutor. We've added a lot of parameters and functionality to AgentExecutor, but its still just one way of running a loop.\\uf8ffüí°We're excited to announce the release of langgraph, a new library to allow for creating language agents as graphs.This will allow users to create far more custom cyclical behavior. You can define\", metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       "  Document(page_content='ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into langchain-community ‚Äì this allows us to centralize integration-specific work. We have also begun to split out individual integrations into their own packages. So far we have done this for ~10 packages, including OpenAI, Google and Mistral. One benefit of this is better dependency management - previously, all dependencies were optional, leading to some headaches when trying to install specific versions. Now if integrations are in their own package, we can more strictly version their requirements, leading to easier installation. Another benefit is versioning. Oftentimes, there are changes to the third party integrations, which require breaking changes. These can now be reflected on an individual integration basis with proper versioning in the standalone integration package.ObservabilityBuilding LLM applications involves putting a non-deterministic component at the center of your system. These models can often output unexpected results, so having visibility into exactly what is happening in your system is integral. \\uf8ffüí°We want to make langchain as observable and as debuggable as possible, whether through architectural decisions or tools we build on the side.We‚Äôve set about this in a few ways.The main way we‚Äôve tackled this is by building LangSmith. One of the main value props that LangSmith provides is a best-in-class debugging experience for your LLM application. We log exactly what steps are happening, what the inputs of each step are, what the outputs of each step are, how long each step takes, and more data. We display this in a user-friendly way, allowing you to identify which steps are taking the longest, enter a playground to debug unexpected LLM responses, track token usage and more. Even in private beta, the demand for LangSmith has been overwhelming, and we‚Äôre investing a lot in scalability so that we can release a public beta and then make it generally available in the coming months. We are also already supporting an enterprise version, which comes with a within-VPC deployment for enterprises with strict data privacy policies.We‚Äôve also tackled observability in other ways. We‚Äôve long had built in verbose and debug modes for different levels of logging throughout the pipeline. We recently introduced methods to visualize the chain you created, as well as get all prompts used.ComposabilityWhile it‚Äôs helpful to have prebuilt chains to get started, we very often see teams breaking outside of those architectures and wanting to customize their chain - not only customize the prompt, but also customize different parts of the orchestration. \\uf8ffüí°Over the past few months, we‚Äôve invested heavily in LangChain Expression Language (LCEL). This enables composition of arbitrary sequences, providing a lot of the same benefits as data orchestration tools do for data engineering pipelines (batching, parallelization, fallbacks). It also provides some benefits unique to LLM workloads - mainly LLM-specific observability (covered above), and streaming, covered later in this post.The components for LCEL are in langchain-core. We‚Äôve started to create higher level entry points for specific chains in LangChain. These will gradually replace pre-existing (now ‚ÄúLegacy‚Äù) chains, because chains built with LCEL will get streaming, ease of customization, observability, batching, retries out-of-the-box. Our goal is to make this transition seamless. Previously you may have done:ConversationalRetrievalChain.from_llm(llm, ‚Ä¶)We want to simply make it:create_conversational_retrieval_chain(llm, ‚Ä¶)Under the hood, it will create a specific LCEL chain and return it. If you want to modify the logic - no problem! Because it‚Äôs all written in LCEL it‚Äôs easy to modify part of it without having to subclass anything or override any methods.There', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'})],\n",
       " 'answer': 'LangChain 0.1.0 is the first stable version of the LangChain framework. It is fully backwards compatible and available in both Python and JavaScript. This release focuses on improving functionality and documentation. The stable version of LangChain aims to earn developer trust and provide a foundation for systematic and safe evolution of the library.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "09rT_Q_rYCgj",
    "outputId": "5c2f3ac1-1e22-4467-cda8-465ebb247c8f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'LangChain 0.1.0 is the first stable version of the LangChain framework. It is fully backwards compatible and available in both Python and JavaScript. This release focuses on improving functionality and documentation. The stable version of LangChain aims to earn developer trust and provide a foundation for systematic and safe evolution of the library.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYRTU-XpYJC3"
   },
   "outputs": [],
   "source": [
    "# simulate conversation history\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Is there anything new about Langchain 0.1.0?\"),\n",
    "    AIMessage(content=\"Yes!\")\n",
    "]\n",
    "\n",
    "response = conversational_retrieval_chain.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    \"input\": \"Tell me more about it!\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAeZs-f4YWrn",
    "outputId": "0f65666d-85ec-45a3-8e34-2084eb2f9d30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Is there anything new about Langchain 0.1.0?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me more about it!',\n",
       " 'context': [Document(page_content='Today we‚Äôre excited to announce the release of langchain 0.1.0, our first stable version. It is fully backwards compatible, comes in both Python and JavaScript, and comes with improved focus through both functionality and documentation. A stable version of LangChain helps us earn developer trust and gives us the ability to evolve the library systematically and safely.Python GitHub DiscussionPython v0.1.0 GuidesJS v0.1.0 GuidesYouTube WalkthroughIntroductionLangChain has been around for a little over a year and has changed a lot as it‚Äôs grown to become the default framework for building LLM applications. As we previewed a month ago, we recently decided to make significant changes to the\\xa0 LangChain package architecture in order to better organize the project and strengthen the foundation.\\xa0Specifically we made two large architectural changes: separating out langchain-core and separating out partner packages (either into langchain-community or standalone partner packages) from langchain.\\xa0As a reminder, langchain-core contains the main abstractions, interfaces, and core functionality. This code is stable and has been following a stricter versioning policy for a little over a month now.langchain itself, however, still remained on 0.0.x versions. Having all releases on minor version 0 created a few challenges:Users couldn‚Äôt be confident that updating would not have breaking changeslangchain became bloated and unstable as we took a ‚Äúmaintain everything‚Äù approach to reduce breaking changes and deprecation notificationsHowever, starting today with the release of langchain 0.1.0, all future releases will follow a new versioning standard. Specifically:Any breaking changes to the public API will result in a minor version bump (the second digit)Any bug fixes or new features will result in a patch version bump (the third digit)We hope that this, combined with the previous architectural changes, will:Communicate clearly if breaking changes are made, allowing developers to update with confidenceGive us an avenue for officially deprecating and deleting old code, reducing bloatMore responsibly deal with integrations (whose SDKs are often changing as rapidly as LangChain)Even after we release a 0.2 version, we will commit to maintaining a branch of 0.1, but will only patch critical bug fixes. See more towards the end of this post on our plans for that.While re-architecting the package towards a path to a stable 0.1 release, we took the opportunity to talk to hundreds of developers about why they use LangChain and what they love about it. This input guided our direction and focus. We also used it as an opportunity to bring parity to the Python and JavaScript versions in the core areas outlined below. \\uf8ffüí°While certain integrations and more tangential chains may be language specific, core abstractions and key functionality are implemented equally in both the Python and JavaScript packages.We want to share what we‚Äôve heard and our plan to continually improve LangChain. We hope that sharing these learnings will increase transparency into our thinking and decisions, allowing others to better use, understand, and contribute to LangChain. After all, a huge part of LangChain is our community ‚Äì both the user base and the 2000+ contributors ‚Äì and we want everyone to come along for the journey.\\xa0Third Party IntegrationsOne of the things that people most love about LangChain is how easy we make it to get started building on any stack. We have almost 700 integrations, ranging from LLMs to vector stores to tools for agents to use. \\uf8ffüí°LangChain is often used as the ‚Äúglue‚Äù to join all the different pieces you need to build an LLM app together, and so prioritizing a robust integration ecosystem is a priority for us.About a month ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       "  Document(page_content='LangChain v0.1.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n                LangChain Blog\\n        \\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nBy LangChain\\n\\n\\n\\n\\nRelease Notes\\n\\n\\n\\n\\nGitHub\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain v0.1.0\\n\\n10 min read\\nJan 8, 2024', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       "  Document(page_content=\"advanced methods while also making retrieval more production ready. We‚Äôve implemented advanced retrieval strategies from academia (like FLARE and Hyde), created our own (like Parent Document and Self-Query), and adapted some from other industry solutions (like Multi-Query - derived from query expansion commonly used in search). We‚Äôve also made sure to support production concerns like per-user retrieval - crucial for any application where you are storing documents for multiple users together.Importantly, while LangChain provides all the necessary components for building advanced retrieval systems, we are not overly opinionated on how to do so. This has led to many other libraries building on top of LangChain to provide a more opinionated approach to retrieval - like EmbedChain and GPTResearcher.AgentsOne of the earliest things that LangChain became known for was agentic workloads. This can mean two things:Tool use: having an LLM call a function or toolReasoning: how to best enable an LLM to call a tool multiple times, and in what order (or not call a tool at all!)On the tool use side, we‚Äôve largely covered the components we see as crucial:Integrations with a large number of third party toolsWays to structure the LLMs response to fit the input schema of those toolsA flexible way to specify custom manners in which those tools should be invoked (LCEL)On the reasoning side, we have a few different ‚ÄúAgent‚Äù methods, which can largely be thought of as an LLM running in a loop, deciding each iteration which (if any) tool it needs to call and then observing the result of that tool. We incorporated ReAct (an early prompting strategy for doing so) from the beginning, and have quickly added in many other types, including ones that use OpenAI Function Calling, ones that use their new tool-calling API, ones optimized for conversation, and more.\\uf8ffüí°Through flexible and extensible tool support and advanced reasoning capabilities, LangChain has the become the default way to enable LLMs to take actions.Similar to retrieval, while LangChain provides the building blocks for agents we've also seen several more opinionated frameworks built on top. A great example of this is CrewAI, which builds on top of LangChain to provide an easier interface for multi-agent workloads.LangChain 0.2Even though we just released LangChain 0.1, we‚Äôre already thinking about 0.2. Some things that are top of mind for us are:Rewriting legacy chains in LCEL (with better streaming and debugging support)Adding new types of chainsAdding new types of agentsImproving our production ingestion capabilitiesRemoving old and unused functionalityImportantly, even though we are excited about removing some of the old and legacy code to make langchain slimmer and more focused, we also want to maintain support for people who are still using the old version. That is why we will maintain 0.1 as a stable branch (patching in critical bug fixes) for at least 3 months after 0.2 release. We plan to do this for every stable release from here on out.And if you've been wanting to get started contributing, there's never been a better time. We recently added a good getting started issue on GitHub if you're looking for a place to start.One More ThingA large part of LangChain v0.1.0 is stability and focus on the core areas outlined above. Now that we've identified the areas people love about LangChain, we can work on adding more advanced and complete tooling there.One of the main things people love about LangChain is it's support for agents. Most agents are largely defined as running an LLM in some sort of a loop. So far, the only way we've had to do that is with AgentExecutor. We've added a lot of parameters and functionality to AgentExecutor, but its still just one way of running a loop.\\uf8ffüí°We're excited to announce the release of langgraph, a new library to allow for creating language agents as graphs.This will allow users to create far more custom cyclical behavior. You can define\", metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'}),\n",
       "  Document(page_content='ago, we started making some changes we think will improve the robustness, stability, scalability, and general developer experience around integrations. We split out ALL third party integrations into langchain-community ‚Äì this allows us to centralize integration-specific work. We have also begun to split out individual integrations into their own packages. So far we have done this for ~10 packages, including OpenAI, Google and Mistral. One benefit of this is better dependency management - previously, all dependencies were optional, leading to some headaches when trying to install specific versions. Now if integrations are in their own package, we can more strictly version their requirements, leading to easier installation. Another benefit is versioning. Oftentimes, there are changes to the third party integrations, which require breaking changes. These can now be reflected on an individual integration basis with proper versioning in the standalone integration package.ObservabilityBuilding LLM applications involves putting a non-deterministic component at the center of your system. These models can often output unexpected results, so having visibility into exactly what is happening in your system is integral. \\uf8ffüí°We want to make langchain as observable and as debuggable as possible, whether through architectural decisions or tools we build on the side.We‚Äôve set about this in a few ways.The main way we‚Äôve tackled this is by building LangSmith. One of the main value props that LangSmith provides is a best-in-class debugging experience for your LLM application. We log exactly what steps are happening, what the inputs of each step are, what the outputs of each step are, how long each step takes, and more data. We display this in a user-friendly way, allowing you to identify which steps are taking the longest, enter a playground to debug unexpected LLM responses, track token usage and more. Even in private beta, the demand for LangSmith has been overwhelming, and we‚Äôre investing a lot in scalability so that we can release a public beta and then make it generally available in the coming months. We are also already supporting an enterprise version, which comes with a within-VPC deployment for enterprises with strict data privacy policies.We‚Äôve also tackled observability in other ways. We‚Äôve long had built in verbose and debug modes for different levels of logging throughout the pipeline. We recently introduced methods to visualize the chain you created, as well as get all prompts used.ComposabilityWhile it‚Äôs helpful to have prebuilt chains to get started, we very often see teams breaking outside of those architectures and wanting to customize their chain - not only customize the prompt, but also customize different parts of the orchestration. \\uf8ffüí°Over the past few months, we‚Äôve invested heavily in LangChain Expression Language (LCEL). This enables composition of arbitrary sequences, providing a lot of the same benefits as data orchestration tools do for data engineering pipelines (batching, parallelization, fallbacks). It also provides some benefits unique to LLM workloads - mainly LLM-specific observability (covered above), and streaming, covered later in this post.The components for LCEL are in langchain-core. We‚Äôve started to create higher level entry points for specific chains in LangChain. These will gradually replace pre-existing (now ‚ÄúLegacy‚Äù) chains, because chains built with LCEL will get streaming, ease of customization, observability, batching, retries out-of-the-box. Our goal is to make this transition seamless. Previously you may have done:ConversationalRetrievalChain.from_llm(llm, ‚Ä¶)We want to simply make it:create_conversational_retrieval_chain(llm, ‚Ä¶)Under the hood, it will create a specific LCEL chain and return it. If you want to modify the logic - no problem! Because it‚Äôs all written in LCEL it‚Äôs easy to modify part of it without having to subclass anything or override any methods.There', metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'title': 'LangChain v0.1.0', 'language': 'en'})],\n",
       " 'answer': 'LangChain 0.1.0 is the first stable version of the LangChain framework. It is fully backwards compatible and comes in both Python and JavaScript versions. This release focuses on improving functionality and documentation to provide a more robust and reliable experience for developers. \\n\\nOne significant change in this release is the re-architecture of the LangChain package. The package has been separated into langchain-core, which contains the main abstractions, interfaces, and core functionality, and langchain-community, which includes third-party integrations. This separation allows for better organization and maintenance of the project.\\n\\nThe new versioning standard for LangChain has also been introduced with this release. Any breaking changes to the public API will result in a minor version bump, while bug fixes and new features will result in a patch version bump. This new versioning standard aims to provide clearer communication of changes and reduce bloat in the library.\\n\\nAdditionally, the release of LangChain 0.1.0 is accompanied by improvements in third-party integrations and observability. The framework now offers better support for integrating with various tools and services, making it easier to build LLM applications. LangSmith, a debugging tool, has been introduced to provide a comprehensive debugging experience for LLM applications.\\n\\nLooking ahead, the LangChain team is already thinking about version 0.2, which will include rewriting legacy chains, adding new types of chains and agents, improving production ingestion capabilities, and removing old and unused functionality. They also plan to maintain the stable branch of version 0.1 for at least 3 months after the release of 0.2 to support users who are still using the older version.\\n\\nOverall, LangChain 0.1.0 represents a significant milestone in the development of the framework, bringing stability, improved functionality, and a clear roadmap for future enhancements.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "FI2NB_yjYc_4",
    "outputId": "5b8a2153-2703-4431-c744-766901cbeab0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"LangChain 0.1.0 is the first stable version of the LangChain framework. It is fully backwards compatible and is available in both Python and JavaScript. This release comes with improved functionality and documentation, providing a more focused and reliable experience for developers. \\n\\nOne significant change in LangChain 0.1.0 is the separation of langchain-core and partner packages. Langchain-core contains the main abstractions, interfaces, and core functionality, while partner packages are either included in langchain-community or exist as standalone partner packages. This architectural change helps organize the project and strengthen its foundation. \\n\\nTo address the challenges posed by previous minor version releases (0.0.x), LangChain now follows a new versioning standard. Any breaking changes to the public API will result in a minor version bump (second digit), while bug fixes and new features will result in a patch version bump (third digit). This versioning standard aims to provide clarity, allow for deprecation of old code, and handle integrations more effectively. \\n\\nEven after the release of version 0.2, LangChain will maintain a branch of version 0.1, focusing only on critical bug fixes. \\n\\nLangChain has also gathered feedback from hundreds of developers to understand why they use the framework and what they love about it. This input has guided LangChain's direction and focus. Additionally, LangChain offers support for third-party integrations, making it easy to build LLM applications on any stack. \\n\\nLooking ahead, the LangChain team is already planning for version 0.2, which will include rewriting legacy chains, adding new types of chains and agents, improving production ingestion capabilities, and removing old and unused functionality. \\n\\nFinally, LangChain is committed to maintaining stability and focus on core areas while providing advanced and complete tooling support. A new library called langgraph has been released, allowing users to create language agents as graphs, enabling more custom cyclical behavior. \\n\\nOverall, LangChain 0.1.0 brings stability, improved integrations, observability, composability, and a clear roadmap for future development.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7jzZ_DEsPH5"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations for finishing this tutorial. For a more in-depth walk-through, you can go to the official [LangChain documentation](https://python.langchain.com/docs/get_started/quickstart).\n",
    "\n",
    "In the meantime, be sure to subscribe to [my YouTube channel](https://www.youtube.com/@alejandro_ao). I upload free tutorials and courses to help you become an AI Engineer."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-root-py312",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "py312 (Local)",
   "language": "python",
   "name": "conda-root-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
