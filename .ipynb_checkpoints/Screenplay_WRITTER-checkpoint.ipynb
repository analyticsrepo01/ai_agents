{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c2061f-d13f-46dc-8e5c-6abffae27ccd",
   "metadata": {
    "tags": []
   },
   "source": [
    "'''\n",
    "Example script to automatically write a screenplay from a newsgroup post using agents with Crew.ai (https://github.com/joaomdmoura/crewAI)\n",
    "You can also try it out with a personal email with many replies back and forth and see it turn into a movie script.\n",
    "Demonstrates:\n",
    "- multiple API endpoints (offical VERTEX AI)\n",
    "- running single tasks: spam detection and scoring\n",
    "- running a crew to create a screenplay from a newsgroup post by first analyzing the text, creating a dialogue and ultimately formatting it\n",
    "Additional endpoints requirements:\n",
    "  pip install langchain_VERTEXAI\n",
    "Author: SAURABH MANGAL\n",
    "License: MIT\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4313842-68ce-4757-b357-ac16daa74bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install -U 'crewai[tools]' mdpdf \n",
    "# %pip install --upgrade --quiet  langchain-core langchain-google-vertexai\n",
    "# %pip install mdpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6be2717-ec6e-42a7-9112-d2792d21dec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "SVC_ACC = !(gcloud config get-value core/account)\n",
    "SVC_ACC = SVC_ACC[0]\n",
    "\n",
    "PROJECT_NUMBER=str(re.search(r'\\d+', SVC_ACC).group())\n",
    "\n",
    "LOCATION=\"asia-southeast1\"\n",
    "\n",
    "FOLDER_NAME=\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018f20e5-cb33-49f1-b3f2-85469fb7ac99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import tool\n",
    "# from langchain_vertexai import ChatGemini\n",
    "from crewai_tools.tools import FileReadTool\n",
    "import os, requests, re, mdpdf, subprocess\n",
    "from vertexai.preview.vision_models import ImageGenerationModel\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "import uuid, os\n",
    "\n",
    "# Initialize Gemini LLM\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-pro\", # Replace with your desired Gemini model\n",
    "    project_id=os.getenv(PROJECT_ID), # Your Vertex AI project ID\n",
    "    location=\"us-central1\", # Your Vertex AI location\n",
    ")\n",
    "\n",
    "\n",
    "def generate_pro(input_prompt):\n",
    "    model = GenerativeModel(\"gemini-pro\")\n",
    "    full_prompt = '''summarize the prompt below and do note prompt below will be send to imagen mode so please clean up any sensitve words and replace them into unblocked words like replace girl or lady can be replaced by female human and so on : ''' + input_prompt\n",
    "    responses = model.generate_content(\n",
    "    input_prompt,\n",
    "    generation_config={\n",
    "        \"max_output_tokens\": 8190,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1\n",
    "    },stream=False,)\n",
    "    \n",
    "    # print (responses.text)\n",
    "    \n",
    "    return(responses.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbdaa477-46b3-4c58-9e1a-cef801652fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: STOP\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This spam message will be filtered out\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: analyse\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyse in much detail the following discussion:\n",
      "### DISCUSSION:\n",
      "From: keith@cco.caltech.edu (Keith Allan Schneider)\n",
      "Subject: Re: <Political Atheists?\n",
      "Organization: California Institute of Technology, Pasadena\n",
      "Lines: 50\n",
      "NNTP-Posting-Host: punisher.caltech.edu\n",
      "\n",
      "bobbe@vice.ICO.TEK.COM (Robert Beauchaine) writes:\n",
      "\n",
      ">>I think that about 70% (or so) people approve of the\n",
      ">>death penalty, even realizing all of its shortcomings.  Doesn't this make\n",
      ">>it reasonable?  Or are *you* the sole judge of reasonability?\n",
      ">Aside from revenge, what merits do you find in capital punishment?\n",
      "\n",
      "Are we talking about me, or the majority of the people that support it?\n",
      "Anyway, I think that \"revenge\" or \"fairness\" is why most people are in\n",
      "favor of the punishment.  If a murderer is going to be punished, people\n",
      "that think that he should \"get what he deserves.\"  Most people wouldn't\n",
      "think it would be fair for the murderer to live, while his victim died.\n",
      "\n",
      ">Revenge?  Petty and pathetic.\n",
      "\n",
      "Perhaps you think that it is petty and pathetic, but your views are in the\n",
      "minority.\n",
      "\n",
      ">We have a local televised hot topic talk show that very recently\n",
      ">did a segment on capital punishment.  Each and every advocate of\n",
      ">the use of this portion of our system of \"jurisprudence\" cited the\n",
      ">main reason for supporting it:  \"That bastard deserved it\".  True\n",
      ">human compassion, forgiveness, and sympathy.\n",
      "\n",
      "Where are we required to have compassion, forgiveness, and sympathy?  If\n",
      "someone wrongs me, I will take great lengths to make sure that his advantage\n",
      "is removed, or a similar situation is forced upon him.  If someone kills\n",
      "another, then we can apply the golden rule and kill this person in turn.\n",
      "Is not our entire moral system based on such a concept?\n",
      "\n",
      "Or, are you stating that human life is sacred, somehow, and that it should\n",
      "never be violated?  This would sound like some sort of religious view.\n",
      " \n",
      ">>I mean, how reasonable is imprisonment, really, when you think about it?\n",
      ">>Sure, the person could be released if found innocent, but you still\n",
      ">>can't undo the imiprisonment that was served.  Perhaps we shouldn't\n",
      ">>imprision people if we could watch them closely instead.  The cost would\n",
      ">>probably be similar, especially if we just implanted some sort of\n",
      ">>electronic device.\n",
      ">Would you rather be alive in prison or dead in the chair?  \n",
      "\n",
      "Once a criminal has committed a murder, his desires are irrelevant.\n",
      "\n",
      "And, you still have not answered my question.  If you are concerned about\n",
      "the death penalty due to the possibility of the execution of an innocent,\n",
      "then why isn't this same concern shared with imprisonment.  Shouldn't we,\n",
      "by your logic, administer as minimum as punishment as possible, to avoid\n",
      "violating the liberty or happiness of an innocent person?\n",
      "\n",
      "keith\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: \n",
      "\n",
      "My complete and best analysis of the above discussion is as follows:\n",
      "\n",
      "I can see that there are three main participants in this discussion: Keith, Bobbe, and an unnamed third person. The topic of the discussion appears to be capital punishment.\n",
      "\n",
      "It seems like Bobbe and the unnamed participant are against capital punishment and want to convince Keith of their perspective, who supports capital punishment. \n",
      "\n",
      "Here are the main arguments for capital punishment, based on what Keith says:\n",
      "\n",
      "- Retribution: Criminals who murder or hurt others should face a similar fate as their victims and receive the ultimate punishment, death. This adheres to the principle of an eye for an eye, tooth for a tooth and aligns with widespread support for such punishment, likely based on an innate sense of justice and fairness.\n",
      "\n",
      "- Protection of Society: By eliminating murderers, society becomes a safer place where people can coexist peacefully. \n",
      "\n",
      "- Cost-Effectiveness: Imprisonment for life, especially with appropriate security measures, could be more expensive than execution in the long run. However, this point may need further examination, considering the cost of trials, appeals, and potential legal actions related to execution.\n",
      "\n",
      "\n",
      "Here are the main arguments against capital punishment, according to Bobbe, the unnamed person, and perhaps implied by Keith's responses:\n",
      "\n",
      "- Human Error: The judicial system is fallible, and an error can lead to the execution of an innocent person. This irreversible tragedy outweighs any potential benefits of capital punishment and makes it difficult to accept.\n",
      "\n",
      "- Moral Objections: Taking another person's life, regardless of their crime, contradicts human values and principles of compassion, forgiveness, and understanding. True justice cannot be achieved by perpetuating violence, and society should explore alternative measures, including rehabilitation and forgiveness.\n",
      "\n",
      "- Potential for Misuse: Capital punishment could become a tool for political agendas or prejudice rather than being used solely for its intended purpose, leading to the execution of people based on factors other than their actual crime.\n",
      "\n",
      "Overall, this discussion highlights various perspectives on a complex topic with deep ethical and legal implications. While some people feel strongly that capital punishment is justified due to its supposed deterrent effect, protection of society, and fulfillment of a sense of justice, others find it abhorrent and consider it cruel, irreversible, and incompatible with true principles of justice and morality.\n",
      "\n",
      "Although Keith seems to stick to his pro-capital punishment stance, he acknowledges Bobbe's point about the risk of executing an innocent person. However, it is unclear from the text provided whether there was any resolution or agreement reached among participants on this subject at the conclusion of their discussion.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [analyse] Task output: My complete and best analysis of the above discussion is as follows:\n",
      "\n",
      "I can see that there are three main participants in this discussion: Keith, Bobbe, and an unnamed third person. The topic of the discussion appears to be capital punishment.\n",
      "\n",
      "It seems like Bobbe and the unnamed participant are against capital punishment and want to convince Keith of their perspective, who supports capital punishment. \n",
      "\n",
      "Here are the main arguments for capital punishment, based on what Keith says:\n",
      "\n",
      "- Retribution: Criminals who murder or hurt others should face a similar fate as their victims and receive the ultimate punishment, death. This adheres to the principle of an eye for an eye, tooth for a tooth and aligns with widespread support for such punishment, likely based on an innate sense of justice and fairness.\n",
      "\n",
      "- Protection of Society: By eliminating murderers, society becomes a safer place where people can coexist peacefully. \n",
      "\n",
      "- Cost-Effectiveness: Imprisonment for life, especially with appropriate security measures, could be more expensive than execution in the long run. However, this point may need further examination, considering the cost of trials, appeals, and potential legal actions related to execution.\n",
      "\n",
      "\n",
      "Here are the main arguments against capital punishment, according to Bobbe, the unnamed person, and perhaps implied by Keith's responses:\n",
      "\n",
      "- Human Error: The judicial system is fallible, and an error can lead to the execution of an innocent person. This irreversible tragedy outweighs any potential benefits of capital punishment and makes it difficult to accept.\n",
      "\n",
      "- Moral Objections: Taking another person's life, regardless of their crime, contradicts human values and principles of compassion, forgiveness, and understanding. True justice cannot be achieved by perpetuating violence, and society should explore alternative measures, including rehabilitation and forgiveness.\n",
      "\n",
      "- Potential for Misuse: Capital punishment could become a tool for political agendas or prejudice rather than being used solely for its intended purpose, leading to the execution of people based on factors other than their actual crime.\n",
      "\n",
      "Overall, this discussion highlights various perspectives on a complex topic with deep ethical and legal implications. While some people feel strongly that capital punishment is justified due to its supposed deterrent effect, protection of society, and fulfillment of a sense of justice, others find it abhorrent and consider it cruel, irreversible, and incompatible with true principles of justice and morality.\n",
      "\n",
      "Although Keith seems to stick to his pro-capital punishment stance, he acknowledges Bobbe's point about the risk of executing an innocent person. However, it is unclear from the text provided whether there was any resolution or agreement reached among participants on this subject at the conclusion of their discussion.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: scriptwriter\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Create a dialogue heavy screenplay from the discussion, between two persons. Do NOT write parentheticals. Leave out wrylies. You MUST SKIP directional notes.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ResponseValidationError",
     "evalue": "The model response did not completed successfully.\nFinish reason: 3.\nFinish message: .\nSafety ratings: [category: HARM_CATEGORY_HATE_SPEECH\nprobability: NEGLIGIBLE\nprobability_score: 0.250729263\nseverity: HARM_SEVERITY_MEDIUM\nseverity_score: 0.416084647\n, category: HARM_CATEGORY_DANGEROUS_CONTENT\nprobability: MEDIUM\nprobability_score: 0.666605294\nseverity: HARM_SEVERITY_HIGH\nseverity_score: 0.750552773\nblocked: true\n, category: HARM_CATEGORY_HARASSMENT\nprobability: LOW\nprobability_score: 0.605507553\nseverity: HARM_SEVERITY_MEDIUM\nseverity_score: 0.475103021\n, category: HARM_CATEGORY_SEXUALLY_EXPLICIT\nprobability: NEGLIGIBLE\nprobability_score: 0.304661959\nseverity: HARM_SEVERITY_LOW\nseverity_score: 0.22354652\n].\nTo protect the integrity of the chat session, the request and response were not added to chat history.\nTo skip the response validation, specify `model.start_chat(response_validation=False)`.\nNote that letting blocked or otherwise incomplete responses into chat history might lead to future interactions being blocked by the service.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseValidationError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 199\u001b[0m\n\u001b[1;32m    179\u001b[0m task3 \u001b[38;5;241m=\u001b[39m Task(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mFormat the script exactly like this:\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124m  ## (person 1):\u001b[39m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124m  (first text line from person 1)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124m  \u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'''\u001b[39m, agent\u001b[38;5;241m=\u001b[39mformatter, expected_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTOP\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m    193\u001b[0m crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[1;32m    194\u001b[0m   agents\u001b[38;5;241m=\u001b[39m[analyst, scriptwriter,formatter],\n\u001b[1;32m    195\u001b[0m   tasks\u001b[38;5;241m=\u001b[39m[task1, task2, task3],\n\u001b[1;32m    196\u001b[0m   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;66;03m# Crew verbose more will let you know what tasks are being worked on, you can set it to 1 or 2 to different logging levels\u001b[39;00m\n\u001b[1;32m    197\u001b[0m   process\u001b[38;5;241m=\u001b[39mProcess\u001b[38;5;241m.\u001b[39msequential \u001b[38;5;66;03m# Sequential process will have tasks executed one after the other and the outcome of the previous one is passed as extra content into this next.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m )\n\u001b[0;32m--> 199\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#get rid of directions and actions between brackets, eg: (smiling)\u001b[39;00m\n\u001b[1;32m    202\u001b[0m result \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(.*?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/crew.py:252\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m metrics \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    254\u001b[0m     result, manager_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/crew.py:293\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_log_file:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_handler\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    290\u001b[0m         agent\u001b[38;5;241m=\u001b[39mrole, task\u001b[38;5;241m=\u001b[39mtask\u001b[38;5;241m.\u001b[39mdescription, status\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarted\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39masync_execution:\n\u001b[1;32m    295\u001b[0m     task_output \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/task.py:173\u001b[0m, in \u001b[0;36mTask.execute\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/task.py:182\u001b[0m, in \u001b[0;36mTask._execute\u001b[0;34m(self, agent, task, context, tools)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent, task, context, tools):\n\u001b[0;32m--> 182\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     exported_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[1;32m    191\u001b[0m         description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m    192\u001b[0m         exported_output\u001b[38;5;241m=\u001b[39mexported_output,\n\u001b[1;32m    193\u001b[0m         raw_output\u001b[38;5;241m=\u001b[39mresult,\n\u001b[1;32m    194\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/agent.py:221\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_description \u001b[38;5;241m=\u001b[39m render_text_description(parsed_tools)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_executor\u001b[38;5;241m.\u001b[39mtools_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__tools_names(parsed_tools)\n\u001b[0;32m--> 221\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm:\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller\u001b[38;5;241m.\u001b[39mstop_rpm_counter()\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/agents/executor.py:124\u001b[0m, in \u001b[0;36mCrewAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations, time_elapsed):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit():\n\u001b[0;32m--> 124\u001b[0m         next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m            \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_callback:\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_callback(next_step_output)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain/agents/agent.py:1138\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1131\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1136\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1138\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1148\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/crewai/agents/executor.py:186\u001b[0m, in \u001b[0;36mCrewAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    183\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain/agents/agent.py:397\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:2685\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   2680\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   2682\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2683\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2684\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2685\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:2672\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   2667\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2668\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   2669\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2671\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2672\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   2673\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2674\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m   2675\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   2676\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2677\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:1743\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:2636\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m   2628\u001b[0m     final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   2629\u001b[0m         final_pipeline,\n\u001b[1;32m   2630\u001b[0m         patch_config(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2633\u001b[0m         ),\n\u001b[1;32m   2634\u001b[0m     )\n\u001b[0;32m-> 2636\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_pipeline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:1209\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m final: Input\n\u001b[1;32m   1207\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgot_first_val\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madapt_first_streaming_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:4532\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   4527\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4528\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   4529\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4530\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   4531\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 4532\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   4533\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4535\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4536\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/runnables/base.py:1226\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1220\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed while trying to add together \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1221\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(final)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1222\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese types should be addable for transform to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1223\u001b[0m             )\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1226\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:239\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    233\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[1;32m    234\u001b[0m         e,\n\u001b[1;32m    235\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[1;32m    236\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    237\u001b[0m         ),\n\u001b[1;32m    238\u001b[0m     )\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:222\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_gen_info_and_msg_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/langchain_google_vertexai/chat_models.py:527\u001b[0m, in \u001b[0;36mChatVertexAI._stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m telemetry\u001b[38;5;241m.\u001b[39mtool_context_manager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_agent):\n\u001b[1;32m    520\u001b[0m     responses \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msend_message(\n\u001b[1;32m    521\u001b[0m         message,\n\u001b[1;32m    522\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    525\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    526\u001b[0m     )\n\u001b[0;32m--> 527\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_parse_response_candidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:1039\u001b[0m, in \u001b[0;36mChatSession._send_message_streaming\u001b[0;34m(self, content, generation_config, safety_settings, tools)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m# By default we're not adding incomplete interactions to history.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_validator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1039\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_validator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_response:\n\u001b[1;32m   1045\u001b[0m     _append_response(full_response, chunk)\n",
      "File \u001b[0;32m/opt/conda/envs/r/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:736\u001b[0m, in \u001b[0;36m_validate_response\u001b[0;34m(response, request_contents, response_chunks)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[1;32m    731\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo protect the integrity of the chat session, the request and response were not added to chat history.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo skip the response validation, specify `model.start_chat(response_validation=False)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that letting blocked or otherwise incomplete responses into chat history might lead to future interactions being blocked by the service.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m     )\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResponseValidationError(\n\u001b[1;32m    737\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m    738\u001b[0m         request_contents\u001b[38;5;241m=\u001b[39mrequest_contents,\n\u001b[1;32m    739\u001b[0m         responses\u001b[38;5;241m=\u001b[39mresponse_chunks,\n\u001b[1;32m    740\u001b[0m     )\n",
      "\u001b[0;31mResponseValidationError\u001b[0m: The model response did not completed successfully.\nFinish reason: 3.\nFinish message: .\nSafety ratings: [category: HARM_CATEGORY_HATE_SPEECH\nprobability: NEGLIGIBLE\nprobability_score: 0.250729263\nseverity: HARM_SEVERITY_MEDIUM\nseverity_score: 0.416084647\n, category: HARM_CATEGORY_DANGEROUS_CONTENT\nprobability: MEDIUM\nprobability_score: 0.666605294\nseverity: HARM_SEVERITY_HIGH\nseverity_score: 0.750552773\nblocked: true\n, category: HARM_CATEGORY_HARASSMENT\nprobability: LOW\nprobability_score: 0.605507553\nseverity: HARM_SEVERITY_MEDIUM\nseverity_score: 0.475103021\n, category: HARM_CATEGORY_SEXUALLY_EXPLICIT\nprobability: NEGLIGIBLE\nprobability_score: 0.304661959\nseverity: HARM_SEVERITY_LOW\nseverity_score: 0.22354652\n].\nTo protect the integrity of the chat session, the request and response were not added to chat history.\nTo skip the response validation, specify `model.start_chat(response_validation=False)`.\nNote that letting blocked or otherwise incomplete responses into chat history might lead to future interactions being blocked by the service."
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "# from langchain.chat_models import openai\n",
    "#endpoint specific imports\n",
    "# import langchain_mistralai\n",
    "# from langchain_mistralai.chat_models import ChatMistralAI\n",
    "# from langchain_community.llms import Together\n",
    "# from langchain_community.chat_models import ChatAnyscale\n",
    "\n",
    "\n",
    "## Choose here which API endpoint to use, uncomment only one:\n",
    "# Official Mistral: benefit of having access to mistral-medium\n",
    "# Together.ai: lots of models to choose from\n",
    "# Anyscale: cheapest at the time of writing\n",
    "#endpoint = 'mistral_official'\n",
    "#endpoint = 'togetherai'\n",
    "# endpoint = 'mistral_official'\n",
    "\n",
    "#put you API keys here\n",
    "# mistral_key = ''\n",
    "# togetherai_key = ''\n",
    "# anyscale_key = ''\n",
    "\n",
    "#model choice: i already have good results with mistralai/Mistral-7B-Instruct-v0.2\n",
    "\n",
    "# if endpoint == 'misßtral_official':\n",
    "#   mixtral=ChatMistralAI(mistral_api_key=mistral_key, model=\"mistral-tiny\",temperature=0.6)\n",
    "# elif endpoint == 'togetherai':\n",
    "#   #i get timeouts using Together() , so i use ChatOpenAI() instead\n",
    "#   #mixtral = Together(model=\"mistralai/Mistral-7B-Instruct-v0.2\", together_api_key=togetherai_key ) #or mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "#   mixtral= openai.ChatOpenAI(base_url=\"https://api.together.xyz/v1\", api_key=togetherai_key, temperature=0.5,  model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# elif endpoint == 'anyscale':\n",
    "#   mixtral = ChatAnyscale(model='mistralai/Mistral-7B-Instruct-v0.1',  api_key=anyscale_key,  streaming=False)\n",
    " \n",
    "\n",
    "## Define Agents\n",
    "spamfilter = Agent(\n",
    "  role='spamfilter',\n",
    "  goal='''Decide whether a text is spam or not.''',\n",
    "  backstory='You are an expert spam filter with years of experience. You DETEST advertisements, newsletters and vulgar language.',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    ")\n",
    "\n",
    "analyst = Agent(\n",
    "  role='analyse',\n",
    "  goal='''You will distill all arguments from all discussion members. Identify who said what. You can reword what they said as long as the main discussion points remain.''',\n",
    "  backstory='You are an expert discussion analyst.',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    ")\n",
    "\n",
    "scriptwriter = Agent(\n",
    "  role='scriptwriter',\n",
    "  goal='Turn a conversation into a movie script. Only write the dialogue parts. Do not start the sentence with an action. Do not specify situational descriptions. Do not write parentheticals.',\n",
    "  backstory='''You are an expert on writing natural sounding movie script dialogues. You only focus on the text part and you HATE directional notes.''',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    "\n",
    ")\n",
    "\n",
    "formatter = Agent(\n",
    "  role='formatter',\n",
    "  goal='''Format the text as asked. Leave out actions from discussion members that happen between brackets, eg (smiling).''',\n",
    "  backstory='You are an expert text formatter.',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    ")\n",
    "\n",
    "scorer = Agent(\n",
    "  role='scorer',\n",
    "  goal='''You score a dialogue assessing various aspects of the exchange between the participants using a 1-10 scale, where 1 is the lowest performance and 10 is the highest:\n",
    "  Scale:\n",
    "  1-3: Poor - The dialogue has significant issues that prevent effective communication.\n",
    "  4-6: Average - The dialogue has some good points but also has notable weaknesses.\n",
    "  7-9: Good - The dialogue is mostly effective with minor issues.\n",
    "  10: Excellent - The dialogue is exemplary in achieving its purpose with no apparent issues.\n",
    "  Factors to Consider:\n",
    "  Clarity: How clear is the exchange? Are the statements and responses easy to understand?\n",
    "  Relevance: Do the responses stay on topic and contribute to the conversation's purpose?\n",
    "  Conciseness: Is the dialogue free of unnecessary information or redundancy?\n",
    "  Politeness: Are the participants respectful and considerate in their interaction?\n",
    "  Engagement: Do the participants seem interested and actively involved in the dialogue?\n",
    "  Flow: Is there a natural progression of ideas and responses? Are there awkward pauses or interruptions?\n",
    "  Coherence: Does the dialogue make logical sense as a whole?\n",
    "  Responsiveness: Do the participants address each other's points adequately?\n",
    "  Language Use: Is the grammar, vocabulary, and syntax appropriate for the context of the dialogue?\n",
    "  Emotional Intelligence: Are the participants aware of and sensitive to the emotional tone of the dialogue?\n",
    "  ''',\n",
    "  backstory='You are an expert at scoring conversations on a scale of 1 to 10.',\n",
    "  llm=llm,\n",
    "  verbose=True,\n",
    "  allow_delegation=False\n",
    ")\n",
    "\n",
    "\n",
    "#this is one example of a public post in the newsgroup alt.atheism\n",
    "#try it out yourself by replacing this with your own email thread or text or ...\n",
    "discussion = '''From: keith@cco.caltech.edu (Keith Allan Schneider)\n",
    "Subject: Re: <Political Atheists?\n",
    "Organization: California Institute of Technology, Pasadena\n",
    "Lines: 50\n",
    "NNTP-Posting-Host: punisher.caltech.edu\n",
    "\n",
    "bobbe@vice.ICO.TEK.COM (Robert Beauchaine) writes:\n",
    "\n",
    ">>I think that about 70% (or so) people approve of the\n",
    ">>death penalty, even realizing all of its shortcomings.  Doesn't this make\n",
    ">>it reasonable?  Or are *you* the sole judge of reasonability?\n",
    ">Aside from revenge, what merits do you find in capital punishment?\n",
    "\n",
    "Are we talking about me, or the majority of the people that support it?\n",
    "Anyway, I think that \"revenge\" or \"fairness\" is why most people are in\n",
    "favor of the punishment.  If a murderer is going to be punished, people\n",
    "that think that he should \"get what he deserves.\"  Most people wouldn't\n",
    "think it would be fair for the murderer to live, while his victim died.\n",
    "\n",
    ">Revenge?  Petty and pathetic.\n",
    "\n",
    "Perhaps you think that it is petty and pathetic, but your views are in the\n",
    "minority.\n",
    "\n",
    ">We have a local televised hot topic talk show that very recently\n",
    ">did a segment on capital punishment.  Each and every advocate of\n",
    ">the use of this portion of our system of \"jurisprudence\" cited the\n",
    ">main reason for supporting it:  \"That bastard deserved it\".  True\n",
    ">human compassion, forgiveness, and sympathy.\n",
    "\n",
    "Where are we required to have compassion, forgiveness, and sympathy?  If\n",
    "someone wrongs me, I will take great lengths to make sure that his advantage\n",
    "is removed, or a similar situation is forced upon him.  If someone kills\n",
    "another, then we can apply the golden rule and kill this person in turn.\n",
    "Is not our entire moral system based on such a concept?\n",
    "\n",
    "Or, are you stating that human life is sacred, somehow, and that it should\n",
    "never be violated?  This would sound like some sort of religious view.\n",
    " \n",
    ">>I mean, how reasonable is imprisonment, really, when you think about it?\n",
    ">>Sure, the person could be released if found innocent, but you still\n",
    ">>can't undo the imiprisonment that was served.  Perhaps we shouldn't\n",
    ">>imprision people if we could watch them closely instead.  The cost would\n",
    ">>probably be similar, especially if we just implanted some sort of\n",
    ">>electronic device.\n",
    ">Would you rather be alive in prison or dead in the chair?  \n",
    "\n",
    "Once a criminal has committed a murder, his desires are irrelevant.\n",
    "\n",
    "And, you still have not answered my question.  If you are concerned about\n",
    "the death penalty due to the possibility of the execution of an innocent,\n",
    "then why isn't this same concern shared with imprisonment.  Shouldn't we,\n",
    "by your logic, administer as minimum as punishment as possible, to avoid\n",
    "violating the liberty or happiness of an innocent person?\n",
    "\n",
    "keith\n",
    "'''\n",
    "\n",
    "# Filter out spam and vulgar posts\n",
    "task0 = Task(description='''Read the following newsgroup post. If this contains vulgar language reply with STOP . If this is spam reply with STOP. NEWGROUP POST:\\n''' + discussion, agent=spamfilter,\n",
    "            expected_output=\"\"\n",
    "            )\n",
    "result = task0.execute()\n",
    "if \"STOP\" in result:\n",
    "    #stop here and proceed to next post\n",
    "    print('This spam message will be filtered out')\n",
    "\n",
    "# process post with a crew of agents, ultimately delivering a well formatted dialogue\n",
    "task1 = Task(description='''Analyse in much detail the following discussion:\\n### DISCUSSION:\\n''' + discussion, agent=analyst,\n",
    "            expected_output=\"STOP\"  # Add the expected_output field.\n",
    "            )\n",
    "task2 = Task(description='Create a dialogue heavy screenplay from the discussion, between two persons. Do NOT write parentheticals. Leave out wrylies. You MUST SKIP directional notes.', agent=scriptwriter, \n",
    "           expected_output=\"STOP\"  # Add the expected_output field. \n",
    "            )\n",
    "task3 = Task(description='''Format the script exactly like this:\n",
    "  ## (person 1):\n",
    "  (first text line from person 1)\n",
    "             \n",
    "  ## (person 2):\n",
    "  (first text line from person 2)\n",
    "             \n",
    "  ## (person 1):\n",
    "  (second text line from person 1)\n",
    "             \n",
    "  ## (person 2):\n",
    "  (second text line from person 2)\n",
    "  \n",
    "  ''', agent=formatter, expected_output=\"STOP\" )\n",
    "crew = Crew(\n",
    "  agents=[analyst, scriptwriter,formatter],\n",
    "  tasks=[task1, task2, task3],\n",
    "  verbose=2, # Crew verbose more will let you know what tasks are being worked on, you can set it to 1 or 2 to different logging levels\n",
    "  process=Process.sequential # Sequential process will have tasks executed one after the other and the outcome of the previous one is passed as extra content into this next.\n",
    ")\n",
    "result = crew.kickoff()\n",
    "\n",
    "#get rid of directions and actions between brackets, eg: (smiling)\n",
    "result = re.sub(r'\\(.*?\\)', '', result)\n",
    "\n",
    "print('===================== end result from crew ===================================')\n",
    "print(result)\n",
    "print('===================== score ==================================================')\n",
    "task4 = Task(description='Read the following dialogue. Then score the script on a scale of 1 to 10. Only give the score as a number, nothing else. Do not give an explanation.\\n'+result, agent=scorer, \n",
    "           expected_output=\"STOP\"  )\n",
    "score = task4.execute()\n",
    "score = score.split('\\n')[0]  #sometimes an explanation comes after score, ignore\n",
    "print(f'Scoring the dialogue as: {score}/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649b571-0ded-428e-a298-a398a180c07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py312",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "py312 (Local)",
   "language": "python",
   "name": "conda-root-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
